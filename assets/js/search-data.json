{
  
    
        "post0": {
            "title": "Deep Networks (Draft)",
            "content": "from copy import deepcopy from sklearn.metrics import r2_score, mean_squared_error import sys sys.path.append(&#39;/notebooks/Mangoes/src/&#39;) model_path = &#39;../models/&#39; . Cross-Validation . So far we have implemented a minimum-viable model and evaluation. We now wrap this into a cross-validation framework. . First we need to consider a key design choice. For our sklearn models, cross-validation led to building multiple versions of a model with slightly different parameters. For neural networks, with optimising being a highly stochastic gradient descent path, there is no gurantee that cross-validation folds are similar, or that folds resemble the final model trained on all the data. After running cross-validation, we let the user define how to build the final model; None, for building no final model to save time, &quot;All&quot;, to train a model on the whole training set, or &quot;Ensemble&quot;, to build an ensemble on the cross-validation folds. . We define our ensemble implementation below. At the moment we just pass in each model. IN the future we may instead pass in a location of a savefile for each model, or a single model and a list of locations to get weights from. . @dataclass class EnsembleConfig(ModelConfig): models: list = [] voting : str = &quot;Mean&quot; class Ensemble(EnsembleModel): def __init__( self, config: DictConfig, **kwargs ): super().__init__(config, **kwargs) def _build_network(self): self.models = self.hparams[&quot;continuous_dim&quot;] def forward(self,x): x = x[&quot;continuous&quot;] y_hats=torch.zeros(x.shape[1],len(self.models)) for i,model in enumerate(models): y_hats[:,i]= self.model.forward(x)[&#39;logits&#39;] if voting == &quot;Mean&quot;: y_hat = torch.mean(y_hats) return {&#39;logits&#39;:y_hat} . from sklearn.model_selection import GroupKFold, KFold def cross_validate(model,X,y,splitter=GroupKFold(),groups=None,plot=False,save_loc=None,final_model=&quot;Ensemble&quot;): #Ensemble, &quot;All&quot;, None #combine X and y Xy = deepcopy(X) Xy[&quot;target&quot;]=y preds = None ys = None models = [] for fold, (inds1,inds2) in enumerate(splitter.split(X,y,groups)): model.fit(X.iloc[inds1,:],y.iloc[inds1,:]) pred = model.predict(X.iloc[inds2,:]) if preds is None: preds = pred ys = y.iloc[inds2,:] else: preds = np.concatenate((preds,pred),axis=0) ys = np.concatenate((ys,y.iloc[inds2,:]),axis=0) if final_model == &quot;Ensemble&quot;: models.append(deepcopy(model)) r2 = r2_score(ys,preds) mse = mean_squared_error(ys,preds) if plot: ys = ys.flatten() preds = preds.flatten() m, b = np.polyfit(ys, preds, 1) fig, ax = plt.subplots() ls = np.linspace(min(ys),max(ys)) ax.plot(ls,ls*m+b,color = &quot;black&quot;, label = r&quot;$ hat{y}$ = &quot;+f&quot;{m:.4f}y + {b:.4f}&quot;) ax.scatter(x=ys,y=preds,label = r&quot;$R^2$&quot; + f&quot;={r2:.4f}&quot;) ax.set_xlabel(&#39;True Values&#39;) ax.set_ylabel(&#39;Predicted Values&#39;) ax.legend(bbox_to_anchor=(0.5,1)) if not save_loc is None: fig.savefig(save_loc) if final_model == &quot;Ensemble&quot;: #create new tabular model, passing in same configs but with an ensemble ens_config = EnsembleConfig(models=models) ensemble = TabularModel( data_config=model.data_config, optimizer_config=model.optimizer_config, trainer_config=model.trainer_config, model_config=ens_config, model_callable = Ensemble ) model = ensemble elif final_model == &quot;All&quot;: #train final model on all data model = model.fit(train=Xy) else: #ignore training the final model, for computation saving purposes model = None return model, mse . def evaluate(model,train_X,train_y,test_X,test_y,plot=False,save_loc=None,log=True): test_y=test_y.values.flatten() model.fit(train_X,train_y) preds = model.predict(test_X) r2 = r2_score(test_y,preds) mse = mean_squared_error(test_y,preds) if log: print(f&quot;Test set MSE: {mse:.4f}&quot;) if plot: preds=preds.flatten() m, b = np.polyfit(test_y, preds, 1) fig, ax = plt.subplots() ls = np.linspace(min(test_y),max(test_y)) ax.plot(ls,ls*m+b,color = &quot;black&quot;, label = r&quot;$ hat{y}$ = &quot;+f&quot;{m:.4f}y + {b:.4f}&quot;) ax.scatter(x=test_y,y=preds,label = r&quot;$R^2$&quot; + f&quot;={r2:.4f}&quot;) ax.set_xlabel(&#39;True Values&#39;) ax.set_ylabel(&#39;Predicted Values&#39;) ax.legend(bbox_to_anchor=(0.5,1)) if not save_loc is None: fig.savefig(save_loc) return model, mse .",
            "url": "https://huonfraser.github.io/Mangoes/mangoes/2022/07/25/Deep_Networks.html",
            "relUrl": "/mangoes/2022/07/25/Deep_Networks.html",
            "date": " • Jul 25, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Deep Networks with Pytorch-Tabular",
            "content": "import sys sys.path.append(&#39;/notebooks/Mangoes/src/&#39;) model_path = &#39;../models/&#39; import pathlib import pandas as pd import numpy as np from matplotlib import pyplot as plt from tqdm.notebook import tqdm from codetiming import Timer from sklearn.model_selection import GroupKFold from scikit_models import * from skopt.space import Real, Integer from lwr import LocalWeightedRegression from sklearn.pipeline import Pipeline import warnings warnings.filterwarnings(&#39;ignore&#39;) . . mangoes=load_mangoes() train_data,test_data = train_test_split(mangoes) train_X, train_y, train_cat = X_y_cat(train_data,min_X=684,max_X=990) test_X, test_y, test_cat = X_y_cat(test_data,min_X=684,max_X=990) nrow,ncol=train_X.shape groups = train_cat[&#39;Pop&#39;] splitter=GroupKFold() . . Pytorch, the neural-network platform of choice for this project requires users to define their own training and validation loops. While this provides excellent flexability, higher-level API&#39;s like pytorch-lighting and fastai cut outmuch of the boilerplate and integrate functionality like callbacks and logging. The pytorch-tabular libary builds upon the pytorch-lightning library to provide an API for dealing with tabular data. . In this notebook we work through building a MLP model. In the next notebook we will cover training and testing in a manner that is consistent with our earlier sklearn models. . import torch import torch.nn as nn import torch.nn.functional as F from omegaconf import DictConfig from typing import Dict from dataclasses import dataclass, field from pytorch_tabular import TabularModel from pytorch_tabular.config import DataConfig,OptimizerConfig, TrainerConfig, ExperimentConfig,ModelConfig from pytorch_tabular.models import BaseModel from collections import OrderedDict from pytorch_tabular.models import CategoryEmbeddingModelConfig . . Defining a Model . Within pytorch-tabular custom networks can be defined by extending the BaseModel class, which in turn extends the pytorch-lightning LightningModule. All hyperparameters are passed in at iniatialisation by the config paramater and is accessable after super() has been called from self.hparams. . Our first step is to write our MLP. We define our network in the _build_network function, consisting of two matrices (linear layers) seperated by a ReLU activation layer. The number of inputs and outputs are controlled by hyperparameters inferred from the data while the width of the hidden layer is a user controlled parameter. . We also are requred to define the forward class, which controls how data is passed through our network. The input of this function x, consists of a dictionary with continuous and categorical features broken down into x[&quot;continuous&quot;] and x[&quot;categorical&quot;]. Outputs of forward must be returned in a dictionary with predictions labelled by &quot;logits&quot;. This is messy (and is unclear in their documentation). Hopefully this is something that will be improved in future iterations of this library. . @dataclass class MLPConfig(ModelConfig): width: int = 10 class MLP(BaseModel): def __init__( self, config: DictConfig, **kwargs ): super().__init__(config, **kwargs) def _build_network(self): layers = OrderedDict({&#39;layer_1&#39;:nn.Linear(self.hparams[&quot;continuous_dim&quot;], self.hparams[&quot;width&quot;]), # &#39;act_1&#39;:nn.ReLU(), &#39;layer_2&#39;:nn.Linear(self.hparams[&quot;width&quot;],self.hparams[&quot;output_dim&quot;]) }) self.model = nn.Sequential(layers) def forward(self,x): x = x[&quot;continuous&quot;] y_hat= self.model.forward(x) return {&#39;logits&#39;:y_hat} . Configurations . Data is expected to be a single pd.DataFram including both X and y. This is a departure from the sklearn approach, and in the future we&#39;ll work on a fix for this. For the time being, we merge our X and y and define as lists the names of our categorical columns, numerical columns, dates, and targets. We pass this metadata into a DataConfig object, which handles loading and transforming data for us. . Similarly we also initialise a TrainerConfig class and an OptimizerConfig class, which between them define all the hyperparmeters controlling training. We also define a ModelConfig, which specifies the parameters that determine how our model is built. . from copy import deepcopy num_col_names = train_X.columns.tolist() cat_col_names = [] train_Xy = deepcopy(train_X) test_Xy = deepcopy(test_X) train_Xy[&#39;target&#39;]=train_y test_Xy[&#39;target&#39;]=test_y data_config = DataConfig( target=[&#39;target&#39;], #target should always be a list. Multi-targets are only supported for regression. Multi-Task Classification is not implemented continuous_cols=num_col_names, categorical_cols=cat_col_names, ) trainer_config = TrainerConfig( auto_lr_find=True, # Runs the LRFinder to automatically derive a learning rate batch_size=32, max_epochs=100, gpus=-1, #index of the GPU to use. -1 means all available GPUs, None, means CPU ) optimizer_config = OptimizerConfig() model_config = MLPConfig(task=&quot;regression&quot;, learning_rate = 1e-3) . All the pieces are assembled in the TabularModel class. We pass in each config class and model_callable, a reference to our MLP class. . tabular_model = TabularModel( data_config=data_config, model_config=model_config, optimizer_config=optimizer_config, trainer_config=trainer_config, model_callable = MLP ) . To train a Tabular Model we call fit, passing in our training data (as a pd.DataFrame) and optionally validation . tabular_model.fit(train=train_Xy, validation=None) . Global seed set to 42 GPU available: True, used: True TPU available: False, using: 0 TPU cores LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] | Name | Type | Params - 0 | model | Sequential | 1.1 K 1 | loss | MSELoss | 0 - 1.1 K Trainable params 0 Non-trainable params 1.1 K Total params 0.004 Total estimated model params size (MB) Global seed set to 42 LR finder stopped early after 99 steps due to diverging loss. Restored states from the checkpoint file at /notebooks/Mangoes/_notebooks/lr_find_temp_model.ckpt Learning rate set to 0.15848931924611143 LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] | Name | Type | Params - 0 | model | Sequential | 1.1 K 1 | loss | MSELoss | 0 - 1.1 K Trainable params 0 Non-trainable params 1.1 K Total params 0.004 Total estimated model params size (MB) Global seed set to 42 . Then to evaluate a model we call evaluate. . tabular_model.evaluate(test_Xy) . LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] . -- DATALOADER:0 TEST RESULTS {&#39;test_mean_squared_error&#39;: 1.9310595989227295, &#39;test_mean_squared_error_0&#39;: 1.9310595989227295} -- . [{&#39;test_mean_squared_error&#39;: 1.9310595989227295, &#39;test_mean_squared_error_0&#39;: 1.9310595989227295}] . Notice that there is a disconnect between the PyTorch-Tabular and scikit-learn apis. In the next part we will work on an implementation that works for the same for both scikit-learn and PyTorch-Tabular models and introduce cross-validation. .",
            "url": "https://huonfraser.github.io/Mangoes/mangoes/2022/06/21/Pytorch_Tabular.html",
            "relUrl": "/mangoes/2022/06/21/Pytorch_Tabular.html",
            "date": " • Jun 21, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Wrapping up Linear Models",
            "content": "import pathlib import pandas as pd import numpy as np import sys sys.path.append(&#39;/notebooks/Mangoes/src/&#39;) model_path = &#39;../models/&#39; from matplotlib import pyplot as plt from tqdm.notebook import tqdm from codetiming import Timer from sklearn.model_selection import GroupKFold from scikit_models import * from skopt.space import Real, Integer from lwr import LocalWeightedRegression from sklearn.pipeline import Pipeline import warnings warnings.filterwarnings(&#39;ignore&#39;) . . mangoes=load_mangoes() train_data,test_data = train_test_split(mangoes) train_X, train_y, train_cat = X_y_cat(train_data,min_X=684,max_X=990) test_X, test_y, test_cat = X_y_cat(test_data,min_X=684,max_X=990) nrow,ncol=train_X.shape groups = train_cat[&#39;Pop&#39;] splitter=GroupKFold() . . Ensemble methods . We now compare our approach to off the shelf ensemble models that are typically the state of the art for tabular problems. We train off-the-shelf variants of Random forest, the default sklearn Gradient boosting regression and XGBoost. A caveat here is that each of these models could probably be optimised further. Intitial performance was dissapointing so we used PLS preprocessing for these experiemnts. . Random Forest . from sklearn.ensemble import RandomForestRegressor model = Pipeline([ (&#39;scaler&#39;, PLSRegression()), (&#39;model&#39;,RandomForestRegressor()) ]) space = [Integer(2,ncol,name=&#39;scaler__n_components&#39;), ] opt = Optimiser(space,model,train_X,train_y,splitter=splitter,groups=groups) model_forest,result_forest = opt.optimise(save_file=model_path+&#39;5_random_forest&#39;) . 100%|██████████| 50/50 [1:50:05&lt;00:00, 243.11s/it] . Best model had an MSE of 1.4690 Setting parameters as: {&#39;scaler__n_components&#39;: 10} Elapsed time: 6632.9616 seconds . Scikit-learn Gradient Boosting . from sklearn.ensemble import GradientBoostingRegressor model = Pipeline([ (&#39;scaler&#39;, PLSRegression()), (&#39;model&#39;,GradientBoostingRegressor(random_state=0)) ]) space = [Integer(2,ncol,name=&#39;scaler__n_components&#39;), ] opt = Optimiser(space,model,train_X,train_y,splitter=splitter,groups=groups) model_boost,result_boost = opt.optimise(save_file=model_path+&#39;5_gradboost&#39;) . 0%| | 0/50 [00:00&lt;?, ?it/s] 2%|▏ | 1/50 [01:20&lt;1:05:27, 80.15s/it] 4%|▍ | 2/50 [03:13&lt;1:19:44, 99.67s/it] 6%|▌ | 3/50 [05:10&lt;1:24:08, 107.41s/it] 8%|▊ | 4/50 [07:04&lt;1:24:21, 110.02s/it] 10%|█ | 5/50 [08:27&lt;1:15:19, 100.43s/it] 12%|█▏ | 6/50 [09:19&lt;1:01:29, 83.84s/it] 14%|█▍ | 7/50 [09:59&lt;49:54, 69.63s/it] 16%|█▌ | 8/50 [10:09&lt;35:31, 50.75s/it] 18%|█▊ | 9/50 [10:47&lt;31:50, 46.59s/it] 20%|██ | 10/50 [11:51&lt;34:38, 51.95s/it] 22%|██▏ | 11/50 [14:05&lt;50:07, 77.12s/it] 24%|██▍ | 12/50 [15:42&lt;52:42, 83.22s/it] 26%|██▌ | 13/50 [16:28&lt;44:18, 71.84s/it] 28%|██▊ | 14/50 [17:25&lt;40:31, 67.53s/it] 30%|███ | 15/50 [18:36&lt;39:57, 68.51s/it] 32%|███▏ | 16/50 [19:35&lt;37:11, 65.63s/it] 34%|███▍ | 17/50 [20:32&lt;34:41, 63.08s/it] 36%|███▌ | 18/50 [22:39&lt;43:52, 82.25s/it] 38%|███▊ | 19/50 [23:40&lt;39:08, 75.76s/it] 40%|████ | 20/50 [24:12&lt;31:18, 62.61s/it] 42%|████▏ | 21/50 [25:58&lt;36:34, 75.66s/it] 44%|████▍ | 22/50 [27:28&lt;37:19, 80.00s/it] 46%|████▌ | 23/50 [27:55&lt;28:49, 64.07s/it] 48%|████▊ | 24/50 [28:18&lt;22:27, 51.84s/it] 50%|█████ | 25/50 [29:33&lt;24:26, 58.67s/it] 52%|█████▏ | 26/50 [31:35&lt;31:05, 77.74s/it] 54%|█████▍ | 27/50 [32:25&lt;26:38, 69.49s/it] 56%|█████▌ | 28/50 [33:01&lt;21:47, 59.44s/it] 58%|█████▊ | 29/50 [34:35&lt;24:21, 69.61s/it] 60%|██████ | 30/50 [35:10&lt;19:48, 59.41s/it] 62%|██████▏ | 31/50 [35:46&lt;16:34, 52.35s/it] 64%|██████▍ | 32/50 [37:57&lt;22:46, 75.92s/it] 66%|██████▌ | 33/50 [38:33&lt;18:08, 64.00s/it] 68%|██████▊ | 34/50 [39:09&lt;14:48, 55.50s/it] 70%|███████ | 35/50 [40:15&lt;14:42, 58.83s/it] 72%|███████▏ | 36/50 [41:43&lt;15:46, 67.60s/it] 74%|███████▍ | 37/50 [43:32&lt;17:19, 79.97s/it] 76%|███████▌ | 38/50 [43:52&lt;12:22, 61.86s/it] 78%|███████▊ | 39/50 [44:26&lt;09:50, 53.66s/it] 80%|████████ | 40/50 [45:10&lt;08:26, 50.64s/it] 82%|████████▏ | 41/50 [45:40&lt;06:39, 44.39s/it] 84%|████████▍ | 42/50 [46:01&lt;04:59, 37.39s/it] 86%|████████▌ | 43/50 [46:55&lt;04:56, 42.41s/it] 88%|████████▊ | 44/50 [47:58&lt;04:51, 48.63s/it] 90%|█████████ | 45/50 [48:49&lt;04:07, 49.41s/it] 92%|█████████▏| 46/50 [50:31&lt;04:20, 65.12s/it] 94%|█████████▍| 47/50 [52:00&lt;03:36, 72.22s/it] 96%|█████████▌| 48/50 [52:37&lt;02:03, 61.64s/it] 98%|█████████▊| 49/50 [53:12&lt;00:53, 53.59s/it] 100%|██████████| 50/50 [54:29&lt;00:00, 60.69s/it] . Best model had an MSE of 1.2088 Setting parameters as: {&#39;scaler__n_components&#39;: 28} Elapsed time: 3305.1457 seconds . XGBoost . import xgboost as xgb model = Pipeline([ (&#39;scaler&#39;, PLSRegression()), (&#39;model&#39;,xgb.XGBRegressor(tree_method=&quot;gpu_hist&quot;)) ]) space = [ Integer(2,ncol,name=&#39;scaler__n_components&#39;) ] opt = Optimiser(space,model,train_X,train_y,splitter=splitter,groups=groups) model_xg, result_xg = opt.optimise(save_file=model_path+&#39;5_xgboost&#39;) . 0%| | 0/50 [00:00&lt;?, ?it/s] 2%|▏ | 1/50 [00:09&lt;07:38, 9.35s/it] 4%|▍ | 2/50 [00:21&lt;08:37, 10.78s/it] 6%|▌ | 3/50 [00:32&lt;08:45, 11.18s/it] 8%|▊ | 4/50 [00:44&lt;08:37, 11.24s/it] 10%|█ | 5/50 [00:53&lt;07:47, 10.39s/it] 12%|█▏ | 6/50 [00:59&lt;06:38, 9.06s/it] 14%|█▍ | 7/50 [01:04&lt;05:36, 7.81s/it] 16%|█▌ | 8/50 [01:07&lt;04:15, 6.09s/it] 18%|█▊ | 9/50 [01:11&lt;03:53, 5.71s/it] 20%|██ | 10/50 [01:19&lt;04:11, 6.29s/it] 22%|██▏ | 11/50 [01:33&lt;05:35, 8.61s/it] 24%|██▍ | 12/50 [01:39&lt;04:53, 7.73s/it] 26%|██▌ | 13/50 [01:49&lt;05:13, 8.48s/it] 28%|██▊ | 14/50 [01:54&lt;04:31, 7.54s/it] 30%|███ | 15/50 [01:59&lt;03:53, 6.66s/it] 32%|███▏ | 16/50 [02:04&lt;03:31, 6.21s/it] 34%|███▍ | 17/50 [02:12&lt;03:42, 6.75s/it] 36%|███▌ | 18/50 [02:17&lt;03:17, 6.18s/it] 38%|███▊ | 19/50 [02:22&lt;03:00, 5.81s/it] 40%|████ | 20/50 [02:26&lt;02:41, 5.37s/it] 42%|████▏ | 21/50 [02:31&lt;02:26, 5.06s/it] 44%|████▍ | 22/50 [02:43&lt;03:27, 7.40s/it] 46%|████▌ | 23/50 [02:51&lt;03:18, 7.36s/it] 48%|████▊ | 24/50 [02:56&lt;02:52, 6.63s/it] 50%|█████ | 25/50 [03:02&lt;02:41, 6.46s/it] 52%|█████▏ | 26/50 [03:06&lt;02:19, 5.79s/it] 54%|█████▍ | 27/50 [03:10&lt;02:04, 5.43s/it] 56%|█████▌ | 28/50 [03:15&lt;01:53, 5.14s/it] 58%|█████▊ | 29/50 [03:26&lt;02:26, 6.98s/it] 60%|██████ | 30/50 [03:31&lt;02:04, 6.23s/it] 62%|██████▏ | 31/50 [03:35&lt;01:49, 5.77s/it] 64%|██████▍ | 32/50 [03:39&lt;01:32, 5.16s/it] 66%|██████▌ | 33/50 [03:44&lt;01:26, 5.08s/it] 68%|██████▊ | 34/50 [03:48&lt;01:18, 4.90s/it] 70%|███████ | 35/50 [03:53&lt;01:12, 4.81s/it] 72%|███████▏ | 36/50 [03:58&lt;01:06, 4.78s/it] 74%|███████▍ | 37/50 [04:06&lt;01:14, 5.76s/it] 76%|███████▌ | 38/50 [04:10&lt;01:04, 5.33s/it] 78%|███████▊ | 39/50 [04:15&lt;00:55, 5.04s/it] 80%|████████ | 40/50 [04:19&lt;00:49, 4.99s/it] 82%|████████▏ | 41/50 [04:24&lt;00:43, 4.87s/it] 84%|████████▍ | 42/50 [04:33&lt;00:49, 6.23s/it] 86%|████████▌ | 43/50 [04:38&lt;00:40, 5.83s/it] 88%|████████▊ | 44/50 [04:41&lt;00:28, 4.83s/it] 90%|█████████ | 45/50 [04:44&lt;00:21, 4.33s/it] 92%|█████████▏| 46/50 [04:59&lt;00:29, 7.47s/it] 94%|█████████▍| 47/50 [05:07&lt;00:22, 7.57s/it] 96%|█████████▌| 48/50 [05:18&lt;00:17, 8.75s/it] 98%|█████████▊| 49/50 [05:31&lt;00:10, 10.12s/it] 100%|██████████| 50/50 [05:35&lt;00:00, 8.18s/it] . Best model had an MSE of 1.2314 Setting parameters as: {&#39;scaler__n_components&#39;: 22} Elapsed time: 339.8149 seconds . Ensembles of PLS-LWR . We&#39;ve left them until last because typically emsembles will always give better performance; any of the models looked at in the previous 3 parts could be ensmbled. We take our previous best model (SG-PLS-LWR) and build a bagging regressor ensemble. This builds 10 copys of a model, with each trained on a sample (with replacement) of the dataset. Predictions are then made by taking the mean of the ensemble. . Results are no better than for the non-ensembled version. A possible explanation is that bagging reduces the density of the feature space, interfering with the locally weighted regressions. . from sklearn.ensemble import BaggingRegressor from joblib import dump, load pipe = load(model_path+&#39;4_pp-pls-lwr_model.joblib&#39;) model = BaggingRegressor(pipe) mse = cross_validate(model,train_X,train_y,splitter=splitter,groups=groups,plot=True) print(f&#39;Train set MSE: {mse}&#39;) . FileNotFoundError Traceback (most recent call last) Input In [8], in &lt;cell line: 4&gt;() 1 from sklearn.ensemble import BaggingRegressor 2 from joblib import dump, load -&gt; 4 pipe = load(model_path+&#39;4_pp-pls-lwr_model.joblib&#39;) 5 model = BaggingRegressor(pipe) 7 mse = cross_validate(model,train_X,train_y,splitter=splitter,groups=groups,plot=True) File /opt/conda/lib/python3.10/site-packages/joblib/numpy_pickle.py:579, in load(filename, mmap_mode) 577 obj = _unpickle(fobj) 578 else: --&gt; 579 with open(filename, &#39;rb&#39;) as f: 580 with _read_fileobject(f, filename, mmap_mode) as fobj: 581 if isinstance(fobj, str): 582 # if the returned file object is a string, this means we 583 # try to load a pickle file generated with an version of 584 # Joblib so we load it with joblib compatibility function. FileNotFoundError: [Errno 2] No such file or directory: &#39;../models/4_pp-pls-lwr_model.joblib&#39; . model, mse_test = evaluate(model,train_X,train_y,test_X,test_y,plot=True) . Comparing Techniques . So far in this series, we started with a multiple linear regression (LR) and dded complexity; feature extraction with partial least squares (PLS), lazy instance weights with locally weighted regressions (LWR) and preprocessing with Savitsky Golay (SG). Adding ach of these components incrementally improved performance during cross-validation, although the hyperparameter settings were not always consistent. The final model in this series (SG-PLS-LWR) gave a cross-validation MSE of 0.7223 and a test MSE of 0.7686. . When we compared this model to off-the-shelf ensembles (including XGBoost) and a bagging-ensemble extension, we found that these underperformed our model. To round out this part of the series we compare our results to the original Mangoes results achieved by Anderson et al. . In the table we below we compare two models by Anderson et al, LPLS, their best performing locally weighted PLS model, and Ensemble, their best ensemble based model to the models we have built in this series. Our approach gave substantially better results that both models. Without going into too much detail, this is likely due to the Anderson et al. models fixing the number of components for PLS to a relatively low number, whereas we kept this hyperparmater flexible. . Model CV Score Test Score . LR | 0.8157 | 1.1147 | . PLS-LR | 0.8116 | - | . LWR | 0.7868 | - | . PLS-LWR | 0.7520 | 0.8113 | . SG-PLS-LWR | 0.7223 | 0.7686 | . E(SG-PLS-LWR) | 0.7236 | 0.7801 | . Anderson et al. LPLS | 0.66 | 0.887 | . Anderson et al. Ensemble | 0.56 | 0.850 | .",
            "url": "https://huonfraser.github.io/Mangoes/mangoes/2022/06/17/Benchmarks.html",
            "relUrl": "/mangoes/2022/06/17/Benchmarks.html",
            "date": " • Jun 17, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Domain Related Preprocessing",
            "content": "So far in this series we have introduced the mangoes dataset and built up to a regression model that uses PLS preprocessing and lazy local instance weights to acheve a MSE (cross-validation score) . Many NIR-spectroscopy applications make use of domain related preprocessing, and we now look at enhancing our model with one of these schemes. . import pathlib import pandas as pd import numpy as np from matplotlib import pyplot as plt from tqdm.notebook import tqdm import sys sys.path.append(&#39;/notebooks/Mangoes/src/&#39;) model_path = &#39;../models/&#39; from codetiming import Timer from sklearn.model_selection import GroupKFold from mangoes_blog.scikit_models import * from skopt.space import Real, Integer #np.random.seed(123) from sklearn.pipeline import Pipeline import warnings warnings.filterwarnings(&#39;ignore&#39;) . . mangoes=load_mangoes() train_data,test_data = train_test_split(mangoes) train_X, train_y, train_cat = X_y_cat(train_data,min_X=684,max_X=990) test_X, test_y, test_cat = X_y_cat(test_data,min_X=684,max_X=990) nrow,ncol=train_X.shape groups = train_cat[&#39;Pop&#39;] splitter=GroupKFold() . . Before we delve into preprocessing methods we write a small tool so that we can visualise the effect of a spectral preprocessing regime. We test this out by showing that standarisation greatly introduces the variance of most spectra points. . from sklearn.preprocessing import StandardScaler def plot_sample(scaler=StandardScaler(), n_samples=5): i = [i for i in range(n_samples)] sample = train_X.iloc[i,:] trans = pd.DataFrame(scaler.fit_transform(train_X)).iloc[i,:] for j,_ in enumerate(i): sample.iloc[j,:].plot(label=f&#39;Raw {j}&#39;,color=&#39;Grey&#39;,legend=True) ax = trans.iloc[j,:].plot(label=f&#39;Scaled {j}&#39;,legend=True) train_X.mean().plot(label=&#39;Raw Mean&#39;, color=&#39;Black&#39;, legend = True) ax.legend(bbox_to_anchor=(1, 1)) plot_sample(n_samples=5) . SNV . The first preprocessing method we look at is standard normal variate (SNV). SNV standaridises each spectra (row), which may allowing models to focus on relative values within each spectra spectra rather than differences in absolute values between spectra. This is simple to implement, we just transpose the column based scaling method. As compared to the raw spectra, SNV increases the amplitude at each end while slightly decreasing ampliutes for the middle portion. . When we test SNV with a linear regression, we find that performance is poor. However, only a very small amount of regularisation is needed. . from sklearn.preprocessing import scale from sklearn.base import TransformerMixin, BaseEstimator class SNV(TransformerMixin, BaseEstimator): def fit(self, X, y=None, sample_weight=None): return self def transform(self, X, copy=None): return scale(X,axis=1) . plot_sample(SNV()) . from sklearn.linear_model import Ridge pls= Pipeline([(&#39;scaler&#39;, SNV()), (&#39;model&#39;, Ridge()) ]) space = [Real(1e-3, 1e3, name=&#39;model__alpha&#39;,prior=&#39;log-uniform&#39;)] opt = Optimiser(space,pls,train_X,train_y,splitter=splitter,groups=groups) model_snv,result_snv = opt.optimise(save_file=model_path+&#39;4_snv&#39;) . Best model had an MSE of 1.1665 Setting parameters as: {&#39;model__alpha&#39;: 0.001} Elapsed time: 26.4898 seconds . MSC . A similar method is multiplicative scatter correction (MSC). Wake the mean spectra ($X_m$)and for each spectra instance ($X_i$) calculate a linear regression $X_i=aX_m+b$. We then inverse this relationship st. we return $ frac{X_i-b}{a}$. Similar to SNV, this transforms data into a measure of deviation from the mean specta. The great thing about these methods is that they are both parameterless. . We test MSC below. Its much slower than SNV (comparable to PLS) while giving even worse performance. While the spectra we examined appear similar to SNV, when we examinging the regression plot, several instances display high errors, which may be the reason for the poor performance. . class MSC(TransformerMixin, BaseEstimator): sample_means = None def fit(self, X, y=None, sample_weight=None): self.sample_means = np.mean(X,axis=0) return self def transform(self, X, copy=None): X_msc = np.zeros_like(X.values) for i in range(X.shape[0]): fit = np.polyfit(self.sample_means, X.values[i,:], 1, full=True) X_msc[i,:] = (X.values[i,:] - fit[0][1]) / fit[0][0] return X_msc . plot_sample(MSC()) . from sklearn.linear_model import Ridge pls= Pipeline([(&#39;scaler&#39;, MSC()), (&#39;model&#39;, Ridge()) ]) space = [Real(1e-3, 1e3, name=&#39;model__alpha&#39;,prior=&#39;log-uniform&#39;)] opt = Optimiser(space,pls,train_X,train_y,splitter=splitter,groups=groups) model_msc,result_msc = opt.optimise(save_file=model_path+&#39;4_msc&#39;) . Best model had an MSE of 2.0044 Setting parameters as: {&#39;model__alpha&#39;: 0.001} Elapsed time: 257.0109 seconds . Savitsky-Golay . The third class of preprocessing we look at is Savitsky-Golay interpolation, which replaces each point with a polynomial fitted to nearby points. This is particlarly useful because derivatives of this polynomial can be taken. An implementation is given in the scipy.signal package. We wrap this in an SKlearn class to be consistent with the other methods. Parameters here are the window length, the order of the polynomial and of the derivative. . from scipy.signal import savgol_filter class SavGol(TransformerMixin, BaseEstimator): def __init__(self, window_length=10, polyorder=2, deriv=0,mode=&#39;interp&#39;): self.window_length=window_length self.polyorder = polyorder self.deriv = deriv self.mode=mode def fit(self, X, y=None, sample_weight=None): return self def transform(self, X, copy=None): return savgol_filter(X,self.window_length, self.polyorder,deriv=self.deriv,mode=self.mode) . SavGol gives spectra very similar to the base spectra. Taking derivatives flattens the spectra and provides emphasis on the edge regions. . plot_sample(SavGol(window_length=17,polyorder=2,deriv=0)) . plot_sample(SavGol(window_length=13,polyorder=2,deriv=1)) . plot_sample(SavGol(window_length=13,polyorder=2,deriv=2)) . To test Savitsky-Golay, we search the order of derivative from {0,1,2} and the kernel width from {3,5,...,21}. The the optimal model taking no derivative. Unsuprisingly, the result (an MSE of 0.8264) is very similar to the base spectra. . from sklearn.linear_model import Ridge from skopt.space import Categorical pls= Pipeline([(&#39;scaler&#39;, SavGol(window_length=13,polyorder=2)), (&#39;model&#39;, Ridge()) ]) space = [Integer(3,21,name=&#39;scaler__window_length&#39;), Integer(0,2,name=&#39;scaler__deriv&#39;), Real(1e-3, 1e3, name=&#39;model__alpha&#39;,prior=&#39;log-uniform&#39;)] opt = Optimiser(space,pls,train_X,train_y,splitter=splitter,groups=groups) model_savgol,result_savgol = opt.optimise(save_file=model_path+&#39;4_savgot&#39;) . Best model had an MSE of 0.8713 Setting parameters as: {&#39;scaler__window_length&#39;: 3, &#39;scaler__deriv&#39;: 0, &#39;model__alpha&#39;: 0.001} Elapsed time: 31.4681 seconds . Preprocessing Search . So far we&#39;ve introduced our preprocessing techniques (SNV, MSC, Sav-Gol) and found they are pretty dismal for a linear regression. However we need to add the caveat that linear regression does fairly well on this dataset, a moderate amount of regularisation gives reuslts similar to PLS; the preprocessing methods may give better results for other learners. . To investigate this we plug in each preprocessing scheme into a PLS-LWR model. We investigate the following schemes. Suffixed numbers indicate the orer of derivative for Savitsky Golay. . SNV | MSC | SavGol-1 | SavGol-2 | SNV - SavGol-1 | SNV - SavGol-2 | . from lwr import LocalWeightedRegression from skopt.space import Categorical pipe = Pipeline([(&#39;preprocess&#39;,StandardScaler()), (&#39;scaler&#39;,PLSRegression()), (&#39;model&#39;,LocalWeightedRegression()) ]) preprocess_options= [SNV(), MSC(), SavGol(window_length=13,polyorder=2,deriv=1), SavGol(window_length=13,polyorder=2,deriv=2), ] space = [Categorical(preprocess_options,name=&#39;preprocess&#39;), Integer(1,ncol,name=&#39;scaler__n_components&#39;), Integer(1,nrow,name=&#39;model__n_neighbours&#39;), Real(1e-3, 1e3, name=&#39;model__alpha&#39;,prior=&#39;log-uniform&#39;) ] opt = Optimiser(space,pipe,train_X,train_y,splitter=splitter,groups=groups) model_1,result_1 = opt.optimise(save_file=model_path+&#39;4_pp-pls-lwr1&#39;) . Best model had an MSE of 0.7536 Setting parameters as: {&#39;preprocess&#39;: SavGol(deriv=1, window_length=13), &#39;scaler__n_components&#39;: 40, &#39;model__n_neighbours&#39;: 2059, &#39;model__alpha&#39;: 0.001} Elapsed time: 6335.7100 seconds . pipe = Pipeline([(&#39;preprocess&#39;,SavGol(window_length=13,polyorder=2,deriv=1)), (&#39;scaler&#39;,PLSRegression()), (&#39;model&#39;,LocalWeightedRegression()) ]) space = [Integer(3,21,name=&#39;preprocess__window_length&#39;), Integer(0,2,name=&#39;preprocess__deriv&#39;), Integer(1,ncol,name=&#39;scaler__n_components&#39;), Integer(1,nrow,name=&#39;model__n_neighbours&#39;), Real(1e-3, 1e3, name=&#39;model__alpha&#39;,prior=&#39;log-uniform&#39;) ] opt = Optimiser(space,pipe,train_X,train_y,splitter=splitter,groups=groups) model_2,result_2 = opt.optimise(save_file=model_path+&#39;4_pp-pls-lwr2&#39;) . Best model had an MSE of 0.7223 Setting parameters as: {&#39;preprocess__window_length&#39;: 9, &#39;preprocess__deriv&#39;: 1, &#39;scaler__n_components&#39;: 71, &#39;model__n_neighbours&#39;: 1285, &#39;model__alpha&#39;: 0.4451583920332214} Elapsed time: 5628.9874 seconds . Summary . In this notebook we have investigated various spectral preprocessing schemes (SNV, MSC, and Savitsky Golay derivatives). We started out by showing how transformed spectra and building linear models on transforemd data. These gave dissapointing results on the mangoes dataset. Using these techniques as an additional preprocessing step for our PLS-LWR model gave more promising results. After first searching across all preprocessing types, the first derivative Savitisky-Golay method gave the most promising results. We refocused our search around this method, finding a set of parameters which set a new best cross-validation MSE of 0.7223. We evaluate this model on the test set below, with an MSE of xyz . model_2,mse_test = evaluate(model_2,train_X,train_y,test_X,test_y,plot=True) . Test set MSE: 0.7686 .",
            "url": "https://huonfraser.github.io/Mangoes/mangoes/2022/06/13/Preprocessing.html",
            "relUrl": "/mangoes/2022/06/13/Preprocessing.html",
            "date": " • Jun 13, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Domain Related Regessions",
            "content": "In this previous part, we introduced the Mangoes data set, performed explanatory analysis for the metadata, defined our evaluation approach and built a linear model. In this part we extend the linear model by using Partial Least Squares (PLS) preprocessing and local sample weightings. . import pathlib import pandas as pd import numpy as np from matplotlib import pyplot as plt from tqdm.notebook import tqdm import sys sys.path.append(&#39;/notebooks/Mangoes/src/&#39;) model_path = &#39;../models/&#39; from codetiming import Timer from sklearn.model_selection import GroupKFold from mangoes_blog.scikit_import * from skopt.space import Real, Integer from lwr import LocalWeightedRegression from sklearn.pipeline import Pipeline import warnings warnings.filterwarnings(&#39;ignore&#39;) . . mangoes=load_mangoes() train_data,test_data = train_test_split(mangoes) train_X, train_y, train_cat = X_y_cat(train_data,min_X=684,max_X=990) test_X, test_y, test_cat = X_y_cat(test_data,min_X=684,max_X=990) nrow,ncol=train_X.shape groups = train_cat[&#39;Pop&#39;] splitter=GroupKFold() . . Partial Least Squares . PLS is like a supervised equivalent to principal components (PCA) and is widely used for IR-spectra analysis. Like PCA, a change of basis is performed with features transformed along orthooganl axes. The difference between the two is that while PCA selects components that explain variance within X, PLS selects components from X that explain variance within y, making features extracted by PLS more suitable for regression. . We make a small change to the sklearn PLS implementation as it doensn&#39;t play nice with the Pipeline class; being a supervised learner, PLS transforms and returns both X and y in a tuple. Future steps in the pipeline expect only the transformed X values. . from sklearn.cross_decomposition import PLSRegression as PLS_ class PLSRegression(PLS_): def transform(self,X,y=None,copy=True): X = super().transform(X,copy=copy) return X . The optimial model takes 30 components, with a moderate amount of $l_2$ regularisation. Search took around 5 minutes (this varies by run), an order of magnitude more than the search run last post. In future approaches we may want to run PLS only once with 30 components so that for PLS we fit and transform the data only once. . from sklearn.linear_model import Ridge pls= Pipeline([(&#39;scaler&#39;, PLSRegression(n_components=10)), (&#39;model&#39;, Ridge()) ]) space = [Integer(2,ncol,name=&#39;scaler__n_components&#39;), Real(1e-3, 1e3, name=&#39;model__alpha&#39;,prior=&#39;log-uniform&#39;) ] opt = Optimiser(space,pls,train_X,train_y,splitter=splitter,groups=groups) model_pls,result_pls = opt.optimise(save_file=model_path+&#39;3_pls&#39;) . Best model had an MSE of 0.8116 Setting parameters as: {&#39;scaler__n_components&#39;: 35, &#39;model__alpha&#39;: 0.27337815806191307} Elapsed time: 277.2199 seconds . Locally Weighted Regression . Another method to improve a linear regression is to use lazyily calcualted local instance weights. Rather than training a single, global, model, we define a model that takes unique weights for each each prediction instance. This can be done any number of ways. The approach we take is to use Euclidan distance between and assign the k closest training instances a weight of 1 and the rest a weight of 0. An intuitiv way to understand this is as a kNN model with voting done by linear regression. At train time, we store the training set. At test time, we calculate our closest k-neighbours on which a linear regression is trained. . The advantage of this local weighting approach is that we can avoid outliers in the training data, which may skew a linear model. This approach also better fits non-linear data, as we can create a globally non-linear model from locally linear parts. The downsides of this approach is that we increases the cost of making predictions, the model is less interpretable, and we can run into neighbourhood effects when data is less dense. . We run search with the number of neighbours k ranging from 1 to $ frac{4}{5}nrow$, the size of each training set in cross validation. Note that this is incredibly inefficient for large values of k. We find that a LWR is totally useless here. . from sklearn.preprocessing import StandardScaler lwr= Pipeline([(&#39;scaler&#39;,StandardScaler()), (&#39;model&#39;,LocalWeightedRegression()) ]) space = [Integer(0,1,name=&#39;scaler__with_mean&#39;), Integer(0,1,name=&#39;scaler__with_std&#39;), Real(1e-3, 1e3, name=&#39;model__alpha&#39;,prior=&#39;log-uniform&#39;), Integer(1, nrow*4/5, name=&#39;model__n_neighbours&#39;) ] opt = Optimiser(space,lwr,train_X,train_y,splitter=splitter,groups=groups) model_lwr,result_lwr= opt.optimise(save_file=model_path+&#39;3_lwr&#39;) . Best model had an MSE of 0.7868 Setting parameters as: {&#39;scaler__with_mean&#39;: 0, &#39;scaler__with_std&#39;: 1, &#39;model__alpha&#39;: 0.0024967728668762105, &#39;model__n_neighbours&#39;: 4668} Elapsed time: 12268.5101 seconds . PLS and LWR . We now combine PLS and LWR models which gives an MSE score of 0.7590. Interestingly our model uses more components (71 rather than 31). . plslwr = Pipeline([(&#39;scaler&#39;,PLSRegression()), (&#39;model&#39;,LocalWeightedRegression()) ]) space = [Integer(1,ncol,name=&#39;scaler__n_components&#39;), Integer(1,nrow*4/5,name=&#39;model__n_neighbours&#39;), Real(1e-3, 1e3, name=&#39;model__alpha&#39;,prior=&#39;log-uniform&#39;) ] opt = Optimiser(space,plslwr,train_X,train_y,splitter=splitter,groups=groups) model_plslwr,result_plslwr = opt.optimise(save_file=model_path+&#39;3_plslwr&#39;) . Best model had an MSE of 0.7521 Setting parameters as: {&#39;scaler__n_components&#39;: 36, &#39;model__n_neighbours&#39;: 1484, &#39;model__alpha&#39;: 0.09840278280801977} Elapsed time: 5939.1907 seconds . Summary . In the previous notebook we fitted a linear regresssion with a level of $l_2$ regularisation determined by search. The result of this was a model that on the test set gave an MSE of 1.1290. This notebook has built upon this model by using PLS feature extraction and lazy sample weights, a type of approach that is popular for NIR-spectroscopy problems. Neither technique gave good performance alone, but combined we set a new benchmark for cross-validation performance. On the test set his approach scored an MSE of 0.8117, an improvement of -0.3173 over the previous notebook. . plslwr, mse_test = evaluate(plslwr,train_X,train_y,test_X,test_y,plot=True) . Test set MSE: 0.8113 . TypeError Traceback (most recent call last) Input In [8], in &lt;cell line: 2&gt;() 1 mse_test = evaluate(plslwr,train_X,train_y,test_X,test_y,plot=True) -&gt; 2 print(f&#34;Test set MSE: {mse_test:.4f}&#34;) TypeError: unsupported format string passed to tuple.__format__ .",
            "url": "https://huonfraser.github.io/Mangoes/mangoes/2022/06/07/PLS.html",
            "relUrl": "/mangoes/2022/06/07/PLS.html",
            "date": " • Jun 7, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Evaluation",
            "content": "In the previous notebook, we introduced the Mangoes data set, performed explanatory analysis for the metadata, and built a linear model. In this notebook we define our evaluation approach for future experiments. We want to avoid contuinously evaluating on test set as then we may be selecting for models that fit this set well due to chance alone. On the other hand, we need to validate the performance of our models at least occasionally. The middleground approach we will take is to select a single model from each notebook to be evaluated on the test set, with this selected by cross-validation on the training data. . The other element of our evaluation approach is search, we need a method to select hyperparameters. As a starting point we use 50 evaluations for each search. As our models get more complex we may increase this budget, or switch to budgetting based on time. . import sys sys.path.append(&#39;/notebooks/Mangoes/src/&#39;) model_path = &#39;../models/&#39; import pathlib import pandas as pd import numpy as np from matplotlib import pyplot as plt from sklearn.model_selection import GroupKFold from scikit_models import * from skopt.space import Real, Integer from lwr import LocalWeightedRegression from sklearn.pipeline import Pipeline import warnings warnings.filterwarnings(&#39;ignore&#39;) . . mangoes=load_mangoes() train_data,test_data = train_test_split(mangoes) train_X, train_y, train_cat = X_y_cat(train_data,min_X=684,max_X=990) test_X, test_y, test_cat = X_y_cat(test_data,min_X=684,max_X=990) nrow,ncol=train_X.shape . . Cross-Validation . A ommon cross-validation method for IR-spectra analysis is leave-one-out (in this case fruit), with leave-out-one-population used in applications on Mangoes. However there are concerns that this leads to overfitting, as its likely than for any one particular fruit/pop there is another very similar fruit/pop (As an aside we could measure this by using $kNN_{k=1}$ models, then measuring the deviation between leave-one-out cross-validation scores and train-test scores). Rather we go with the standard 5-fold cross-validation which is a good middle gorund of asking if we can repeat a good level of performance 5 times with decent sized test/validate populations. . The sklearn cross-validate functions (cross_validate, cross_val_score), take averages of scores. This is very practial, as it allows them to nest an evaluation function, however returning functions of scores is not ideal. A better method is to combine the test predictions of each fold into a single set and score based of these. For mean squared error (MSE) this doesn&#39;t make a difference, but for an $R^2$ score, performance may be completely different. It also allows us to plot our results. A requirement for this to work is that each instance in a training set is used for testing only once (s.t. there is one unique prediction for each instance). . Below we define our cross-validation function and an accompanying function to evaluate a model on the test data. . from sklearn.metrics import r2_score, mean_squared_error from sklearn.model_selection import GroupKFold, KFold def cross_validate(model,X,y,splitter=GroupKFold(),groups=None,plot=False,save_loc=None): preds = None ys = None for fold, (inds1,inds2) in enumerate(splitter.split(X,y,groups)): model.fit(X.iloc[inds1,:],y.iloc[inds1,:]) pred = model.predict(X.iloc[inds2,:]) if preds is None: preds = pred ys = y.iloc[inds2,:] else: preds = np.concatenate((preds,pred),axis=0) ys = np.concatenate((ys,y.iloc[inds2,:]),axis=0) r2 = r2_score(ys,preds) mse = mean_squared_error(ys,preds) if plot: ys = ys.flatten() preds = preds.flatten() m, b = np.polyfit(ys, preds, 1) fig, ax = plt.subplots() ls = np.linspace(min(ys),max(ys)) ax.plot(ls,ls*m+b,color = &quot;black&quot;, label = r&quot;$ hat{y}$ = &quot;+f&quot;{m:.4f}y + {b:.4f}&quot;) ax.scatter(x=ys,y=preds,label = r&quot;$R^2$&quot; + f&quot;={r2:.4f}&quot;) ax.set_xlabel(&#39;True Values&#39;) ax.set_ylabel(&#39;Predicted Values&#39;) ax.legend(bbox_to_anchor=(0.5,1)) if not save_loc is None: fig.savefig(save_loc) return mse def evaluate(model,train_X,train_y,test_X,test_y,plot=False,save_loc=None,log=True): test_y=test_y.values.flatten() model.fit(train_X,train_y) preds = model.predict(test_X) r2 = r2_score(test_y,preds) mse = mean_squared_error(test_y,preds) if log: print(f&quot;Test set MSE: {mse:.4f}&quot;) if plot: preds=preds.flatten() m, b = np.polyfit(test_y, preds, 1) fig, ax = plt.subplots() ls = np.linspace(min(test_y),max(test_y)) ax.plot(ls,ls*m+b,color = &quot;black&quot;, label = r&quot;$ hat{y}$ = &quot;+f&quot;{m:.4f}y + {b:.4f}&quot;) ax.scatter(x=test_y,y=preds,label = r&quot;$R^2$&quot; + f&quot;={r2:.4f}&quot;) ax.set_xlabel(&#39;True Values&#39;) ax.set_ylabel(&#39;Predicted Values&#39;) ax.legend(bbox_to_anchor=(0.5,1)) if not save_loc is None: fig.savefig(save_loc) return model, mse . Grouping instance . We use grouped KFold, with grouping done by Pop. As an exercise we show the effect of grouping by comparing to groups by FruitID and no grouping. Mixing populations between folds (grouping by fruit or no grouping) significantly improves validation performance. As the test set includes different populations, this won&#39;t effect the results of the final model, however it may effect our decision of model to select. From here onwards we use grouping by Pop. . Our specific problem is to build a model based of the given populations that can extrapolate to unseen populations, so validating wtih groups based of Pop will give a better indication of the true performance than other methods. . from sklearn.linear_model import LinearRegression model = LinearRegression() #group by pop groups = train_data[&#39;Pop&#39;] splitter = GroupKFold(n_splits=5) print(f&#39;Grouped by Pop, MSE = {cross_validate(model,train_X,train_y,splitter=splitter,groups=groups)}&#39;) #group by Fruit_ID groups = train_data[&#39;Fruit_ID&#39;] splitter = GroupKFold(n_splits=5) print(f&#39;Grouped by Fruit, MSE = {cross_validate(model,train_X,train_y,splitter=splitter,groups=groups)}&#39;) #group by splitter= KFold(shuffle=True) print(f&#39;No Grouping, MSE = {cross_validate(model,train_X,train_y,splitter=splitter,groups=groups)}&#39;) . Grouped by Pop, MSE = 0.8236810194689146 Grouped by Fruit, MSE = 0.6800524561711946 No Grouping, MSE = 0.6748251329051893 . Search . Our task for search is to define a search procudure. Since we&#39;ve defined a custom cross-valdiation method, the sklearn grid_search_cv and random_search cv won&#39;t work. Luckily the scikit-optimize (skopt) package provides a more powerful interface for Bayesian optimisation that we can take advantage of. . We create an interface for the the skopt package, by first defining objective function, which takes a search space and returns a score based on cross-validation results. Any extra parameters for cross validation are defined in initialisation. We then define an optimise function, which wraps our search with nice things like tqdm progress bars and optional outputs This function returns a copy of the model with parameters set to found solution and a dictionary of the search results. All the heavy lifting here is done by the gp_minimize function, which runs Bayesian optimsiation based around a Gaussian process regression kernel. In the future we could extend this to use different kernels. . To create our grid-search and random-search methods, we cheat a little and use the initialisation methods from skopt. Random is just that, while grid search will devide the parameter space evenly to match the number of calls. This approach allows us to run Bayesian, random, and grid search using the same definitions of inputs and recieving the same outputs. . from tqdm.notebook import tqdm from codetiming import Timer from skopt import gp_minimize,dump from skopt.space import Real, Integer from skopt.plots import plot_convergence from skopt.utils import use_named_args from skopt.callbacks import VerboseCallback class TqdmCallback(tqdm): def __call__(self, res): super().update() def __getstate__(self): return [] def __setstate__(self, state): pass class Optimiser(): def __init__(self,space,model,X,y,splitter=KFold(),groups=None): self.space=space self.model=model self.X=X self.y=y self.splitter=splitter self.groups=groups def objective(self,**params): self.model.set_params(**params) return cross_validate(self.model, self.X, self.y, splitter=self.splitter,groups=self.groups) def bayesian_optimise(self,n_calls=50,random_state=0): obj = use_named_args(self.space)(self.objective) return gp_minimize(obj,self.space,n_calls=n_calls,random_state=random_state,callback=TqdmCallback(total=n_calls)) def random_optimise(self,n_calls=50, random_state=0): obj = use_named_args(self.space)(self.objective) return gp_minimize(obj,self.space,n_calls=n_calls,n_initial_points=n_calls,initial_point_generator=&#39;random&#39;, random_state=random_state,callback=TqdmCallback(total=n_calls)) def grid_optimise(self,n_calls=50, random_state=0): obj = use_named_args(self.space)(self.objective) return gp_minimize(obj,self.space,n_calls=n_calls,n_initial_points=n_calls,initial_point_generator=&#39;grid&#39;, random_state=random_state,callback=TqdmCallback(total=n_calls)) @Timer() def optimise(self,strategy=&quot;bayesian&quot;,n_calls=50, random_state=0,plot=True,save_file=None,log=True): if strategy==&quot;bayesian&quot;: result = self.bayesian_optimise(n_calls=n_calls, random_state=random_state) elif strategy==&quot;grid&quot;: result = self.grid_optimise(n_calls=n_calls, random_state=random_state) elif strategy==&quot;random&quot;: result = self.random_optimise(n_calls=n_calls, random_state=random_state) #set parameters of model params = {dim.name:result[&#39;x&#39;][i] for i,dim in enumerate(self.space)} model = self.model.set_params(**params) #save model and search if not save_file is None: del result.specs[&#39;args&#39;][&#39;func&#39;] #spaghetti code to not throw an error as the objective function is unserialisable dump(result,save_file+&#39;_search.pkl&#39;) dump(model, save_file+&#39;_model.joblib&#39;) #plot if plot: plot_convergence(result) # log/print results and include a regression plot: if log: print(f&#39;Best model had an MSE of {result.fun:.4f}&#39;) print(f&#39;Setting parameters as: {params}&#39;) if save_file is None: cross_validate(model, self.X,self.y,splitter=self.splitter,groups=self.groups,plot=True,save_loc=None) else: cross_validate(model, self.X,self.y,splitter=self.splitter,groups=self.groups,plot=True,save_loc=save_file+&#39;_plot.png&#39;) return model,result . Search for L2 Regularisation . We run each search variant below for the problem of finding an appropriate level of $l_2$ regularisation to add to a linear regression alongside a standardisation scheme. . To run our search we first define a model, which should be an sklearn type regression model. We then define our search space, with each dimension defined using one of skopt&#39;s provided classes (Integer, Float, Categorical). For each dimension we define the the range of values. For categorical variables this is an explicit, list, while for real values (floats and integers) we defined the start and end range. We can also define log$_{10}$ sampling for numerical values. . from sklearn.preprocessing import StandardScaler from sklearn.linear_model import Ridge pipe = Pipeline([(&#39;scaler&#39;,StandardScaler()), (&#39;model&#39;,Ridge()) ]) space = [ Integer(0,1,name=&#39;scaler__with_mean&#39;), Integer(0,1,name=&#39;scaler__with_std&#39;), Real(1e-3, 1e3, name=&#39;model__alpha&#39;,prior=&#39;log-uniform&#39;) ] groups = train_data[&#39;Pop&#39;] splitter=GroupKFold() opt = Optimiser(space,pipe,train_X,train_y,splitter=splitter,groups=groups) . We demonstrate results for each class of search below. After our optimiser class is declared we can run multiple searches with no overhead. If we are unhappy with the results we can increase the number of calls or attempt a different seed. . print(&#39;Running Grid Search&#39;) model_grid,result_grid= opt.optimise(strategy=&quot;grid&quot;,save_file=model_path+&#39;2_linear_grid&#39;) . Running Grid Search . 100%|██████████| 50/50 [00:09&lt;00:00, 2.79it/s] . Best model had an MSE of 0.8157 Setting parameters as: {&#39;scaler__with_mean&#39;: 1, &#39;scaler__with_std&#39;: 1, &#39;model__alpha&#39;: 0.005623413251903491} Elapsed time: 11.0227 seconds . print(&#39;Running Random Search&#39;) model_rand,result_rand = opt.optimise(strategy=&quot;random&quot;,save_file=model_path+&#39;2_linear_random&#39;) . Running Random Search . 0%| | 0/50 [00:00&lt;?, ?it/s] 2%|▏ | 1/50 [00:00&lt;00:08, 5.92it/s] 4%|▍ | 2/50 [00:00&lt;00:09, 5.17it/s] 6%|▌ | 3/50 [00:00&lt;00:07, 6.07it/s] 8%|▊ | 4/50 [00:00&lt;00:07, 5.86it/s] 10%|█ | 5/50 [00:00&lt;00:07, 5.69it/s] 12%|█▏ | 6/50 [00:01&lt;00:07, 6.05it/s] 14%|█▍ | 7/50 [00:01&lt;00:07, 5.73it/s] 16%|█▌ | 8/50 [00:01&lt;00:07, 5.49it/s] 18%|█▊ | 9/50 [00:01&lt;00:07, 5.39it/s] 20%|██ | 10/50 [00:01&lt;00:07, 5.53it/s] 22%|██▏ | 11/50 [00:01&lt;00:07, 5.44it/s] 24%|██▍ | 12/50 [00:02&lt;00:06, 5.85it/s] 26%|██▌ | 13/50 [00:02&lt;00:06, 5.96it/s] 28%|██▊ | 14/50 [00:02&lt;00:06, 5.67it/s] 30%|███ | 15/50 [00:02&lt;00:05, 5.88it/s] 32%|███▏ | 16/50 [00:02&lt;00:05, 5.94it/s] 34%|███▍ | 17/50 [00:02&lt;00:05, 6.18it/s] 36%|███▌ | 18/50 [00:03&lt;00:05, 5.44it/s] 38%|███▊ | 19/50 [00:03&lt;00:05, 5.36it/s] 40%|████ | 20/50 [00:03&lt;00:05, 5.87it/s] 42%|████▏ | 21/50 [00:03&lt;00:05, 5.79it/s] 44%|████▍ | 22/50 [00:03&lt;00:05, 5.55it/s] 46%|████▌ | 23/50 [00:04&lt;00:04, 5.79it/s] 48%|████▊ | 24/50 [00:04&lt;00:04, 5.62it/s] 50%|█████ | 25/50 [00:04&lt;00:04, 5.74it/s] 52%|█████▏ | 26/50 [00:04&lt;00:04, 5.68it/s] 54%|█████▍ | 27/50 [00:04&lt;00:03, 5.90it/s] 56%|█████▌ | 28/50 [00:04&lt;00:03, 5.92it/s] 58%|█████▊ | 29/50 [00:05&lt;00:03, 5.74it/s] 60%|██████ | 30/50 [00:05&lt;00:03, 6.10it/s] 62%|██████▏ | 31/50 [00:05&lt;00:03, 5.86it/s] 64%|██████▍ | 32/50 [00:05&lt;00:03, 5.92it/s] 66%|██████▌ | 33/50 [00:05&lt;00:02, 6.25it/s] 68%|██████▊ | 34/50 [00:05&lt;00:02, 6.00it/s] 70%|███████ | 35/50 [00:06&lt;00:02, 6.45it/s] 72%|███████▏ | 36/50 [00:06&lt;00:02, 6.13it/s] 74%|███████▍ | 37/50 [00:06&lt;00:02, 5.74it/s] 76%|███████▌ | 38/50 [00:06&lt;00:01, 6.13it/s] 78%|███████▊ | 39/50 [00:06&lt;00:01, 5.88it/s] 80%|████████ | 40/50 [00:06&lt;00:01, 5.78it/s] 82%|████████▏ | 41/50 [00:07&lt;00:01, 5.69it/s] 84%|████████▍ | 42/50 [00:07&lt;00:01, 5.57it/s] 86%|████████▌ | 43/50 [00:07&lt;00:01, 5.80it/s] 88%|████████▊ | 44/50 [00:07&lt;00:01, 5.73it/s] 90%|█████████ | 45/50 [00:07&lt;00:00, 5.83it/s] 92%|█████████▏| 46/50 [00:07&lt;00:00, 6.14it/s] 94%|█████████▍| 47/50 [00:08&lt;00:00, 6.18it/s] 96%|█████████▌| 48/50 [00:08&lt;00:00, 6.13it/s] 98%|█████████▊| 49/50 [00:08&lt;00:00, 6.08it/s] 100%|██████████| 50/50 [00:09&lt;00:00, 3.05it/s] . Best model had an MSE of 0.8180 Setting parameters as: {&#39;scaler__with_mean&#39;: 0, &#39;scaler__with_std&#39;: 1, &#39;model__alpha&#39;: 0.019920586766671106} Elapsed time: 10.6795 seconds . print(&#39;Running Bayesian Search&#39;) model_bayes,result_bayes= opt.optimise(strategy=&quot;bayesian&quot;,save_file=model_path+&#39;2_linear_bayes&#39;) . Running Bayesian Search . 0%| | 0/50 [00:00&lt;?, ?it/s] 2%|▏ | 1/50 [00:00&lt;00:08, 6.11it/s] 4%|▍ | 2/50 [00:00&lt;00:08, 5.80it/s] 6%|▌ | 3/50 [00:00&lt;00:07, 6.41it/s] 8%|▊ | 4/50 [00:00&lt;00:07, 5.81it/s] 10%|█ | 5/50 [00:00&lt;00:08, 5.47it/s] 12%|█▏ | 6/50 [00:01&lt;00:08, 5.34it/s] 14%|█▍ | 7/50 [00:01&lt;00:08, 5.29it/s] 16%|█▌ | 8/50 [00:01&lt;00:08, 5.06it/s] 18%|█▊ | 9/50 [00:01&lt;00:07, 5.17it/s] 20%|██ | 10/50 [00:02&lt;00:11, 3.40it/s] 22%|██▏ | 11/50 [00:02&lt;00:13, 2.88it/s] 24%|██▍ | 12/50 [00:03&lt;00:14, 2.56it/s] 26%|██▌ | 13/50 [00:03&lt;00:15, 2.45it/s] 28%|██▊ | 14/50 [00:04&lt;00:15, 2.37it/s] 30%|███ | 15/50 [00:04&lt;00:15, 2.32it/s] 32%|███▏ | 16/50 [00:04&lt;00:15, 2.26it/s] 34%|███▍ | 17/50 [00:05&lt;00:14, 2.21it/s] 36%|███▌ | 18/50 [00:05&lt;00:14, 2.22it/s] 38%|███▊ | 19/50 [00:06&lt;00:15, 2.03it/s] 40%|████ | 20/50 [00:06&lt;00:14, 2.03it/s] 42%|████▏ | 21/50 [00:07&lt;00:15, 1.91it/s] 44%|████▍ | 22/50 [00:08&lt;00:14, 1.96it/s] 46%|████▌ | 23/50 [00:08&lt;00:13, 1.97it/s] 48%|████▊ | 24/50 [00:09&lt;00:13, 1.93it/s] 50%|█████ | 25/50 [00:09&lt;00:13, 1.83it/s] 52%|█████▏ | 26/50 [00:10&lt;00:12, 1.89it/s] 54%|█████▍ | 27/50 [00:10&lt;00:11, 1.93it/s] 56%|█████▌ | 28/50 [00:11&lt;00:11, 1.88it/s] 58%|█████▊ | 29/50 [00:11&lt;00:12, 1.75it/s] 60%|██████ | 30/50 [00:12&lt;00:12, 1.64it/s] 62%|██████▏ | 31/50 [00:13&lt;00:12, 1.56it/s] 64%|██████▍ | 32/50 [00:14&lt;00:11, 1.51it/s] 66%|██████▌ | 33/50 [00:14&lt;00:11, 1.47it/s] 68%|██████▊ | 34/50 [00:15&lt;00:11, 1.40it/s] 70%|███████ | 35/50 [00:16&lt;00:10, 1.44it/s] 72%|███████▏ | 36/50 [00:16&lt;00:09, 1.52it/s] 74%|███████▍ | 37/50 [00:17&lt;00:09, 1.41it/s] 76%|███████▌ | 38/50 [00:18&lt;00:08, 1.48it/s] 78%|███████▊ | 39/50 [00:18&lt;00:07, 1.50it/s] 80%|████████ | 40/50 [00:19&lt;00:07, 1.42it/s] 82%|████████▏ | 41/50 [00:20&lt;00:06, 1.48it/s] 84%|████████▍ | 42/50 [00:20&lt;00:05, 1.49it/s] 86%|████████▌ | 43/50 [00:21&lt;00:04, 1.50it/s] 88%|████████▊ | 44/50 [00:22&lt;00:04, 1.41it/s] 90%|█████████ | 45/50 [00:23&lt;00:03, 1.41it/s] 92%|█████████▏| 46/50 [00:23&lt;00:02, 1.41it/s] 94%|█████████▍| 47/50 [00:24&lt;00:02, 1.37it/s] 96%|█████████▌| 48/50 [00:25&lt;00:01, 1.40it/s] 98%|█████████▊| 49/50 [00:25&lt;00:00, 1.44it/s] 100%|██████████| 50/50 [00:26&lt;00:00, 1.41it/s] . Best model had an MSE of 0.8157 Setting parameters as: {&#39;scaler__with_mean&#39;: 1, &#39;scaler__with_std&#39;: 1, &#39;model__alpha&#39;: 0.006159146375736338} Elapsed time: 28.0659 seconds . Results across search were indifferent to mean centring, but scaled variance and added a small amount of $l_2$ regularisation. Bayesian and random search converge much quicker than grid search, with Bayesian search returning the best result by a small margin. . Summary . This notebook has introduced our cross validation and search schemes. We used these search schemes to find an optimal $l_2$ regularisation and standardisation scheme. The selected model gave a training MSE of 0.8156. On the test set (shown below) this model scores an MSE of 1.091, a slight improvement upon the 1.1290 (-0.0380) scored by the base linear regression from the last notebook. . model_bayes,mse_test = evaluate(model_bayes,train_X,train_y,test_X,test_y,plot=True,log=True) . Test set MSE: 1.1147 .",
            "url": "https://huonfraser.github.io/Mangoes/mangoes/2022/06/03/Evaluation.html",
            "relUrl": "/mangoes/2022/06/03/Evaluation.html",
            "date": " • Jun 3, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Introduction",
            "content": "Deep learning for tabular data is experiencing a slight renaissance. Several methods have emerged that can compete to boosting methods (such as XGBoost) in favourable conditions. NIR-spectrometry is a branch of tabular data that typically has two properties, large dimensionality and multicollinearity, which in theory make it suited to deep networks. Multilayer Perceptron networks have been used for decades, and recently CNN architectures have been proposed, however, linear regression models and domain related preprocessing are still the predominant technique. For a detailed review of the NIR-spectra I defer to Anderson and Walsh. . Applications in Industry use huge amounts of test instances, the horticuture industry for iinstance automated grading in horticure packhouses. These analysis rely on prior lab analysis for labelling data. As this analysis is relatively expensive (especially for large amounts of data), most datasets are proprietry. Enter the Mangoes dataset (Anderson et al.), a large NIR-spectroscopy dataset for estimating the relationship between spectra and the dry matter, a measure of maturity of mango fruit. . This is first part of a series comparing a range of deep networks to classical approaches the Mangoes data set. The advantage of the Mangoes dataset is that models (and performance metrics) are publicly available for both classical (Anderson et al.) and deep network (Mishra and Passos) approaches. We can start by (re-)producing similar work and once results are inline we can veer off. The plan is to start with simple linear models and gradually adding complexity. Likel we will do the same for deep networks. Currently the following parts are sketched in, with the plan for deep networks being less clear: . 1) Introduction - Exploratory data analysis and a first linear model. 2) Evaluation - Setup our cross validation and search methods. 3) Spectral Regressions - Cover regression techniques commonly used for spectral applications. 4) Spectral Preprocessing - Cover preprocessing techniques commonly used for spectral applications. 5) Black Box tabular - Compare results to ensembles like RandomForest and XGGoost. 6) Deep Networks ... . This notebook introduces the Mangoes data set and performs expoloratory data analysis. These are done in a literate programming style with text, code and outputs alongside each other. We use the nbdev library, code blocks marked with #export are reused in future notebooks, allowing us to incrementally build a library of tools. Occasionally code segments are deemed too complex/large for a notebook and these will be available in the github repository. . Setup . We begin with imports and library settings which we will use for all notebooks. . import pathlib import pandas as pd import numpy as np from matplotlib import pyplot as plt from sklearn.pipeline import Pipeline np.random.seed(123) import warnings warnings.filterwarnings(&#39;ignore&#39;) . . RuntimeError Traceback (most recent call last) File __init__.pxd:942, in numpy.import_array() RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf During handling of the above exception, another exception occurred: ImportError Traceback (most recent call last) File /opt/conda/lib/python3.10/site-packages/pandas/__init__.py:30, in &lt;module&gt; 29 try: &gt; 30 from pandas._libs import hashtable as _hashtable, lib as _lib, tslib as _tslib 31 except ImportError as e: # pragma: no cover 32 # hack but overkill to use re File /opt/conda/lib/python3.10/site-packages/pandas/_libs/__init__.py:13, in &lt;module&gt; 1 __all__ = [ 2 &#34;NaT&#34;, 3 &#34;NaTType&#34;, (...) 9 &#34;Interval&#34;, 10 ] &gt; 13 from pandas._libs.interval import Interval 14 from pandas._libs.tslibs import ( 15 NaT, 16 NaTType, (...) 21 iNaT, 22 ) File pandas/_libs/interval.pyx:1, in init pandas._libs.interval() File pandas/_libs/hashtable.pyx:1, in init pandas._libs.hashtable() File pandas/_libs/missing.pyx:1, in init pandas._libs.missing() File /opt/conda/lib/python3.10/site-packages/pandas/_libs/tslibs/__init__.py:30, in &lt;module&gt; 29 from . import dtypes &gt; 30 from .conversion import OutOfBoundsTimedelta, localize_pydatetime 31 from .dtypes import Resolution File pandas/_libs/tslibs/conversion.pyx:1, in init pandas._libs.tslibs.conversion() File pandas/_libs/tslibs/nattype.pyx:27, in init pandas._libs.tslibs.nattype() File __init__.pxd:944, in numpy.import_array() ImportError: numpy.core.multiarray failed to import The above exception was the direct cause of the following exception: ImportError Traceback (most recent call last) Input In [1], in &lt;cell line: 3&gt;() 1 #collapse-hide 2 import pathlib -&gt; 3 import pandas as pd 4 import numpy as np 5 from matplotlib import pyplot as plt File /opt/conda/lib/python3.10/site-packages/pandas/__init__.py:34, in &lt;module&gt; 31 except ImportError as e: # pragma: no cover 32 # hack but overkill to use re 33 module = str(e).replace(&#34;cannot import name &#34;, &#34;&#34;) &gt; 34 raise ImportError( 35 f&#34;C extension: {module} not built. If you want to import &#34; 36 &#34;pandas from the source directory, you may need to run &#34; 37 &#34;&#39;python setup.py build_ext --inplace --force&#39; to build the C extensions first.&#34; 38 ) from e 40 from pandas._config import ( 41 get_option, 42 set_option, (...) 46 options, 47 ) 49 # let init-time option registration happen ImportError: C extension: numpy.core.multiarray failed to import not built. If you want to import pandas from the source directory, you may need to run &#39;python setup.py build_ext --inplace --force&#39; to build the C extensions first. . Next we define methods to parse the Mangoes data set. We write a method to load the data (load_mangoes). Training and test splits are provided in the data (for reproducability) and we write a method to seperate these (train_test_split). We then write a method to seperate the spectra data, the target, and other variables into seperate dataframes, with the option to limit the spectra to certain wavelengths. More details about these partitions will be provided below. . def load_mangoes(): mangoes = pd.read_csv(&quot;../data/mangoes_raw.csv&quot;) unique_spectra = mangoes[&#39;DM&#39;].unique() fruit_id_dict = {u:i for i,u in enumerate(unique_spectra)} mangoes[&#39;Fruit_ID&#39;] = mangoes[&#39;DM&#39;].apply(lambda x: fruit_id_dict[x]) return mangoes def train_test_split(data): train_inds = np.logical_not(data[&#39;Set&#39;]==&#39;Val Ext&#39;) test_inds = data[&#39;Set&#39;]==&#39;Val Ext&#39; train_data = data[train_inds] test_data = data[test_inds] return train_data, test_data def X_y_cat(data,min_X=285,max_X=1200): cat_vars=[&#39;Set&#39;,&#39;Season&#39;,&#39;Region&#39;,&#39;Date&#39;,&#39;Type&#39;,&#39;Cultivar&#39;,&#39;Pop&#39;,&#39;Temp&#39;,&#39;Fruit_ID&#39;] y_vars = [&#39;DM&#39;] X_vars = [i for i in data.columns if (not i in y_vars) and (not i in cat_vars)] X_vars = [i for i in X_vars if (int(i)&gt;= min_X) and (int(i)&lt;= max_X)] return data[X_vars], data[y_vars], data[cat_vars] . Running these methods we see that the dataset has 11691 instances, 10243 of which are marked for training and 1448 for testing. . mangoes = load_mangoes() train_data,test_data = train_test_split(mangoes) train_X, train_y, train_cat= X_y_cat(train_data) test_X, test_y, test_cat = X_y_cat(test_data) nrow,ncol=train_X.shape print(f&#39;Data shape: {mangoes.shape}&#39;) print(f&#39;Train data shape: {train_data.shape}&#39;) print(f&#39;Test data shape: {test_data.shape}&#39;) . Taking a step back, below we show a sample of the entire dataset. The first 8 columns are metadata and the 9th, DM, is the target variable. The remaining columns are the spectras, correspnding to readings over 3nm intervals from 285-1200nm. Multiple readings are made for each mango fruit. Each spectra is included twice, corresponding to two spectra readings. Several fruit are read more than twice, for example multiple readings are made at different temperatures. Above we assigned a &#39;Fruit_ID&#39; column based on unique values of DM. This isn&#39;t pefect, different fruit may have the same DM at the precision present in the dataset, or fruit may have different DM values but may be a useful proxy. . print(mangoes.head()) . We also show a selection of spectra from the training portion of the dataset. Missing values in the dataset are coded as 0. Values are missing at each end of the spectrum and are coded as 0. Several columns are completely 0, indicated by the flat lines at each of the below figure. Other columns are partially 0, likely due to different instruments being used to measure different groups of samples. The range of data which includes no missing data is 513-1050nm. Indidently, this is the range that cuts of much of the noise seen at each end. . ax=train_X.mean().plot(label=&#39;Mean&#39;,legend=True) for i in range(0,4): train_X.iloc[i,:].plot(label=i,legend=True) . We also show the distributuon of the target (DM) for the non-test partitions of the data. This is fairly symmetrical and no outliers are present. . train_y.plot.hist(bins=50,density=True) . Anderson et al. used the 684-990nm range for their analysis, so for the sake of comparison we will also use this range of wavelengths. This range gives significantly smoother data. . train_X, train_y, train_cat = X_y_cat(train_data,min_X=684,max_X=990) test_X, test_y, test_cat = X_y_cat(test_data,min_X=684,max_X=990) print(f&#39;Number of selected spectra variables: {train_X.shape[1]}&#39;) ax=train_X.mean().plot(label=&#39;Mean&#39;,legend=True) for i in range(0,4): train_X.iloc[i,:].plot(label=i,legend=True) . Expoloratory Data Analysis . We now dive into all the non-spectra data present in the dataset. We do this for the training data as we want to avoid learning about the test data set for now. We look at distributions of the target and the mean spectra for each categorical variable. . The Set variable deliminates data into training (cal), tuning (tuning) and test (val ext) sets. Previously we used this variable by combining the cal and tuning sets into a combined training data set. Each set has near identical mean spectra; indicating that these splits can be replaced by any random sampling method. DM distributions vary slightly. . train_data[&#39;DM&#39;].hist(by=train_data[&#39;Set&#39;],bins=30,sharey=True,sharex=True,density =True) train_data.groupby(&#39;Set&#39;)[train_X.columns].mean().transpose().plot() train_data[&#39;Set&#39;].value_counts() . Data is taken over 4 growing Seasons. The 4th season is used for the test set, with seasons 1-3 grouped and then divided into training and tuning sets. As for set, pectra readings from season are consistent, while DM varies slightly. . train_data[&#39;DM&#39;].hist(by=train_data[&#39;Season&#39;],bins=30,sharey=True,sharex=True,density =True) train_data.groupby(&#39;Season&#39;)[train_X.columns].mean().transpose().plot() train_data[&#39;Season&#39;].value_counts() . The two growing Regions, NT and QD also display similar spectra readings with slihgtly different DM distributions. . train_data[&#39;DM&#39;].hist(by=train_data[&#39;Region&#39;],bins=30,sharey=True,sharex=True,density =True) train_data.groupby(&#39;Region&#39;)[train_X.columns].mean().transpose().plot() train_data[&#39;Region&#39;].value_counts() . The Physiological stage, or Type, Hard Green or Ripened significantly influences spectra readings and ripened fruit having a much tighter DM distribution. . train_data[&#39;DM&#39;].hist(by=train_data[&#39;Type&#39;],bins=30,sharey=True,sharex=True,density =True) train_data.groupby(&#39;Type&#39;)[train_X.columns].mean().transpose().plot() train_data[&#39;Type&#39;].value_counts() . Each Cultivar displays a slightly different mean spectrs; cultivars display more variance at the low end and converge towards the high end. DM distributins vary highly. . train_data[&#39;DM&#39;].hist(by=train_data[&#39;Cultivar&#39;],bins=30,sharey=True,sharex=True,density =True) train_data.groupby(&#39;Cultivar&#39;)[train_X.columns].mean().transpose().plot() train_data[&#39;Cultivar&#39;].value_counts() . Mangoes are taken from 94 different Populations (orchards). This variable imacts spectra readings to a greater degree than cultivar. Its likely that there is significant overlap between the cultivar and population variables. . train_data.groupby(&#39;Pop&#39;)[train_X.columns].mean().transpose().plot(legend=False) train_data[&#39;Pop&#39;].value_counts() . Readings at different Temperature show the same spectra reading. DM results are identical for each temeperature, which is expected as they share the same lab based label. . train_data[&#39;DM&#39;].hist(by=train_data[&#39;Temp&#39;],bins=30,sharey=True,sharex=True,density =True) train_data.groupby(&#39;Temp&#39;)[train_X.columns].mean().transpose().plot() train_data[&#39;Temp&#39;].value_counts() . Explaining Variance . Having examined each categorical variable in turn we now perform a simple variance analysis. We build a linear model related DM to categorical variables. This is done for each combination of categorical variable (with temperature and date excluded), so that by using $R^2$ scores we can calculate the marginal contribution (to percent of variance explained) by each categorical variable. . Iterating through each combination of variables, we see that Pop explains 0.5474% of the variance. Including Cultivar adds an extra 0.002% for a toal of 0.5476%. We conclude that the effect of Region, Type, and Season variables are solely due to aggregates of Pop, or conversely, that the Pop variable aborbs all of the information of these variables. . from sklearn.preprocessing import OneHotEncoder from sklearn.linear_model import LinearRegression from itertools import combinations for i in range(1,6): oh_vars=[&#39;Season&#39;,&#39;Region&#39;,&#39;Type&#39;,&#39;Cultivar&#39;,&#39;Pop&#39;] for selected in list(combinations(oh_vars,i)): selected = list(selected) enc = OneHotEncoder() enc.fit(train_data[selected]) oh_vars1 = enc.transform(train_data[selected]) lin = LinearRegression() lin.fit(oh_vars1,train_y) score = lin.score(oh_vars1,train_y) print(f&#39;R2 score is {score:.4f} for {selected}&#39;) . To further analyse this we group the data by pop. With the exception of Pop 52, which contains both KP, and LadyG cultivar, each population includes just a single unique value for each categorical value. Variables like Region or Cultivar which may well effect dry matter, we just cannot measure this as their variance is absorbed by Pop. If we wanted to compare say Cultivar and Pop we would need a dataset containing multiple cultivars for each population. . mangoes.groupby(&#39;Pop&#39;).agg({cat_var : &#39;unique&#39; for cat_var in train_cat}) . A Linear Model and Summary . This notebook has introduced the mangoes dataset and performed simple expoloratory analysis, finding that the Pop variable explains over half the variance in the dataset. This has implications for test set performance, with the training and test sets consisting of distinct sets of populations. . In the next few notebooks we will get into the meat and potatoes of building regression models. To finish this notebook off we train a linear model on th training portion of the data, which gives an MSE score of 1.1290 on the test set. . from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error model = LinearRegression() model.fit(train_X,train_y) preds = model.predict(test_X) mse = mean_squared_error(test_y,preds) print(f&quot;Test set MSE is: {mse:.4f}&quot;) .",
            "url": "https://huonfraser.github.io/Mangoes/mangoes/2022/06/01/Introduction.html",
            "relUrl": "/mangoes/2022/06/01/Introduction.html",
            "date": " • Jun 1, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://huonfraser.github.io/Mangoes/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://huonfraser.github.io/Mangoes/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}