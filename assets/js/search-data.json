{
  
    
        "post0": {
            "title": "Wrapping up Linear Models",
            "content": "import pathlib import pandas as pd import numpy as np from matplotlib import pyplot as plt from tqdm.notebook import tqdm from codetiming import Timer from sklearn.model_selection import GroupKFold from mangoes_blog.scikit_models import * from skopt.space import Real, Integer from lwr import LocalWeightedRegression from sklearn.pipeline import Pipeline import warnings warnings.filterwarnings(&#39;ignore&#39;) . . mangoes=load_mangoes() train_data,test_data = train_test_split(mangoes) train_X, train_y, train_cat = X_y_cat(train_data,min_X=684,max_X=990) test_X, test_y, test_cat = X_y_cat(test_data,min_X=684,max_X=990) nrow,ncol=train_X.shape groups = train_cat[&#39;Pop&#39;] splitter=GroupKFold() . . Ensemble methods . We now compare our approach to off the shelf ensemble models that are typically the state of the art for tabular problems. We train off-the-shelf variants of Random forest, the default sklearn Gradient boosting regression and XGBoost. A caveat here is that each of these models could probably be optimised further. Intitial performance was dissapointing so we used PLS preprocessing for these experiemnts. . Random Forest . from sklearn.ensemble import RandomForestRegressor model = Pipeline([ (&#39;scaler&#39;, PLSRegression()), (&#39;model&#39;,RandomForestRegressor()) ]) space = [Integer(2,ncol,name=&#39;scaler__n_components&#39;), ] opt = Optimiser(space,model,train_X,train_y,splitter=splitter,groups=groups) model_forest,result_forest = opt.optimise(save_file=&#39;models/5_random_forest&#39;) . Best model had an MSE of 1.4784963212716935 Setting parameters as: {&#39;scaler__n_components&#39;: 11} Elapsed time: 5373.6494 seconds . Boosting . from sklearn.ensemble import GradientBoostingRegressor model = Pipeline([ (&#39;scaler&#39;, PLSRegression()), (&#39;model&#39;,GradientBoostingRegressor(random_state=0)) ]) space = [Integer(2,ncol,name=&#39;scaler__n_components&#39;), ] opt = Optimiser(space,model,train_X,train_y,splitter=splitter,groups=groups) model_boost,result_boost = opt.optimise(save_file=&#39;models/5_boost&#39;) . Best model had an MSE of 1.2087649202904645 Setting parameters as: {&#39;scaler__n_components&#39;: 28} Elapsed time: 2673.4774 seconds . XGBoost . import xgboost as xgb model = Pipeline([ (&#39;scaler&#39;, PLSRegression()), (&#39;model&#39;,xgb.XGBRegressor(tree_method=&quot;gpu_hist&quot;)) ]) space = [ Integer(2,ncol,name=&#39;scaler__n_components&#39;) ] opt = Optimiser(space,model,train_X,train_y,splitter=splitter,groups=groups) model_xg, result_xg = opt.optimise(save_file=&#39;models/5_xgboost&#39;) . Best model had an MSE of 1.2314341179508546 Setting parameters as: {&#39;scaler__n_components&#39;: 22} Elapsed time: 323.7012 seconds . Ensembles of PLS-LWR . We&#39;ve left them until last because typically emsembles will always give better performance; any of the models looked at in the previous 3 parts could be ensmbled. We take our previous best model (SG-PLS-LWR) and build a bagging regressor ensemble. This builds 10 copys of a model, with each trained on a sample (with replacement) of the dataset. Predictions are then made by taking the mean of the ensemble. . Results are no better than for the non-ensembled version. A possible explanation is that bagging reduces the density of the feature space, interfering with the locally weighted regressions. . from sklearn.ensemble import BaggingRegressor from joblib import dump, load pipe = load(&#39;models/4_pp-pls-lwr_model.joblib&#39;) model = BaggingRegressor(pipe) mse = cross_validate(model,train_X,train_y,splitter=splitter,groups=groups,plot=True) print(f&#39;Train set MSE: {mse}&#39;) . Train set MSE: 0.7236336462319299 . model, mse_test = evaluate(model,train_X,train_y,test_X,test_y,plot=True) . Test set MSE: 0.7801 . Comparing Techniques . So in this series, starting with a linear regression (LR), we have added complexity; feature extraction with partial least squares (PLS), lazy instance weights with locally weighted regressions (LWR) and preprocessing with Savitsky Golay (SG). Adding ach of these components incrementally improved performance during cross-validation, although the hyperparameter settings were not always consistent. The final model in this series (SG-PLS-LWR) gave a cross-validation MSE of 0.7223 and a test MSE of 0.7686. . When we compared this model to off-the-shelf ensembles (including XGBoost) and a bagging-ensemble extension, we found that these underperformed our model. To round out this part of the series we compare our results to those achieved by Anderson et al. We look at two of their models, LPLS, their best performing locally weighted PLS model, and Ensemble, their best ensemble based model. Our approach gave substantially better results on both models. Without going into too much detail, this is likely due to the Anderson et al. models fixing the number of components for PLS to a relatively low number, whereas we kept this hyperparmater flexible. . Model CV Score Test Score . LR | 0.8157 | 1.1147 | . PLS-LR | 0.8116 | - | . LWR | 0.7868 | - | . PLS-LWR | 0.7520 | 0.8113 | . SG-PLS-LWR | 0.7223 | 0.7686 | . E(SG-PLS-LWR) | 0.7236 | 0.7801 | . Anderson et al. LPLS | 0.66 | 0.887 | . Anderson et al. Ensemble | 0.56 | 0.850 | .",
            "url": "https://huonfraser.github.io/Mangoes/mangoes/2022/06/17/Benchmarks.html",
            "relUrl": "/mangoes/2022/06/17/Benchmarks.html",
            "date": " • Jun 17, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Domain Related Preprocessing",
            "content": "So far in this series we have introduced the mangoes dataset and built up to a regression model that uses PLS preprocessing and lazy local instance weights to acheve a MSE (cross-validation score) . Many NIR-spectroscopy applications make use of domain related preprocessing, and we now look at enhancing our model with one of these schemes. . import pathlib import pandas as pd import numpy as np from matplotlib import pyplot as plt from tqdm.notebook import tqdm from codetiming import Timer from sklearn.model_selection import GroupKFold from mangoes_blog.scikit_models import * from skopt.space import Real, Integer #np.random.seed(123) from sklearn.pipeline import Pipeline import warnings warnings.filterwarnings(&#39;ignore&#39;) . . mangoes=load_mangoes() train_data,test_data = train_test_split(mangoes) train_X, train_y, train_cat = X_y_cat(train_data,min_X=684,max_X=990) test_X, test_y, test_cat = X_y_cat(test_data,min_X=684,max_X=990) nrow,ncol=train_X.shape groups = train_cat[&#39;Pop&#39;] splitter=GroupKFold() . . Before we delve into preprocessing methods we write a small tool so that we can visualise the effect of a spectral preprocessing regime. We test this out by showing that standarisation greatly introduces the variance of most spectra points. . from sklearn.preprocessing import StandardScaler def plot_sample(scaler=StandardScaler(), n_samples=5): i = [i for i in range(n_samples)] sample = train_X.iloc[i,:] trans = pd.DataFrame(scaler.fit_transform(train_X)).iloc[i,:] for j,_ in enumerate(i): sample.iloc[j,:].plot(label=f&#39;Raw {j}&#39;,color=&#39;Grey&#39;,legend=True) ax = trans.iloc[j,:].plot(label=f&#39;Scaled {j}&#39;,legend=True) train_X.mean().plot(label=&#39;Raw Mean&#39;, color=&#39;Black&#39;, legend = True) ax.legend(bbox_to_anchor=(1, 1)) plot_sample(n_samples=5) . SNV . The first preprocessing method we look at is standard normal variate (SNV). SNV standaridises each spectra (row), which may allowing models to focus on relative values within each spectra spectra rather than differences in absolute values between spectra. This is simple to implement, we just transpose the column based scaling method. As compared to the raw spectra, SNV increases the amplitude at each end while slightly decreasing ampliutes for the middle portion. . When we test SNV with a linear regression, we find that performance is poor. However, only a very small amount of regularisation is needed. . from sklearn.preprocessing import scale from sklearn.base import TransformerMixin, BaseEstimator class SNV(TransformerMixin, BaseEstimator): def fit(self, X, y=None, sample_weight=None): return self def transform(self, X, copy=None): return scale(X,axis=1) . plot_sample(SNV()) . from sklearn.linear_model import Ridge pls= Pipeline([(&#39;scaler&#39;, SNV()), (&#39;model&#39;, Ridge()) ]) space = [Real(1e-3, 1e3, name=&#39;model__alpha&#39;,prior=&#39;log-uniform&#39;)] opt = Optimiser(space,pls,train_X,train_y,splitter=splitter,groups=groups) model_snv,result_snv = opt.optimise(save_file=&#39;models/4_snv&#39;) . Best model had an MSE of 1.1665 Setting parameters as: {&#39;model__alpha&#39;: 0.001} Elapsed time: 26.4898 seconds . MSC . A similar method is multiplicative scatter correction (MSC). Wake the mean spectra ($X_m$)and for each spectra instance ($X_i$) calculate a linear regression $X_i=aX_m+b$. We then inverse this relationship st. we return $ frac{X_i-b}{a}$. Similar to SNV, this transforms data into a measure of deviation from the mean specta. The great thing about these methods is that they are both parameterless. . We test MSC below. Its much slower than SNV (comparable to PLS) while giving even worse performance. While the spectra we examined appear similar to SNV, when we examinging the regression plot, several instances display high errors, which may be the reason for the poor performance. . class MSC(TransformerMixin, BaseEstimator): sample_means = None def fit(self, X, y=None, sample_weight=None): self.sample_means = np.mean(X,axis=0) return self def transform(self, X, copy=None): X_msc = np.zeros_like(X.values) for i in range(X.shape[0]): fit = np.polyfit(self.sample_means, X.values[i,:], 1, full=True) X_msc[i,:] = (X.values[i,:] - fit[0][1]) / fit[0][0] return X_msc . plot_sample(MSC()) . from sklearn.linear_model import Ridge pls= Pipeline([(&#39;scaler&#39;, MSC()), (&#39;model&#39;, Ridge()) ]) space = [Real(1e-3, 1e3, name=&#39;model__alpha&#39;,prior=&#39;log-uniform&#39;)] opt = Optimiser(space,pls,train_X,train_y,splitter=splitter,groups=groups) model_msc,result_msc = opt.optimise(save_file=&#39;models/4_msc&#39;) . Best model had an MSE of 2.0044 Setting parameters as: {&#39;model__alpha&#39;: 0.001} Elapsed time: 257.0109 seconds . Savitsky-Golay . The third class of preprocessing we look at is Savitsky-Golay interpolation, which replaces each point with a polynomial fitted to nearby points. This is particlarly useful because derivatives of this polynomial can be taken. An implementation is given in the scipy.signal package. We wrap this in an SKlearn class to be consistent with the other methods. Parameters here are the window length, the order of the polynomial and of the derivative. . from scipy.signal import savgol_filter class SavGol(TransformerMixin, BaseEstimator): def __init__(self, window_length=10, polyorder=2, deriv=0,mode=&#39;interp&#39;): self.window_length=window_length self.polyorder = polyorder self.deriv = deriv self.mode=mode def fit(self, X, y=None, sample_weight=None): return self def transform(self, X, copy=None): return savgol_filter(X,self.window_length, self.polyorder,deriv=self.deriv,mode=self.mode) . SavGol gives spectra very similar to the base spectra. Taking derivatives flattens the spectra and provides emphasis on the edge regions. . plot_sample(SavGol(window_length=17,polyorder=2,deriv=0)) . plot_sample(SavGol(window_length=13,polyorder=2,deriv=1)) . plot_sample(SavGol(window_length=13,polyorder=2,deriv=2)) . To test Savitsky-Golay, we search the order of derivative from {0,1,2} and the kernel width from {3,5,...,21}. The the optimal model taking no derivative. Unsuprisingly, the result (an MSE of 0.8264) is very similar to the base spectra. . from sklearn.linear_model import Ridge from skopt.space import Categorical pls= Pipeline([(&#39;scaler&#39;, SavGol(window_length=13,polyorder=2)), (&#39;model&#39;, Ridge()) ]) space = [Integer(3,21,name=&#39;scaler__window_length&#39;), Integer(0,2,name=&#39;scaler__deriv&#39;), Real(1e-3, 1e3, name=&#39;model__alpha&#39;,prior=&#39;log-uniform&#39;)] opt = Optimiser(space,pls,train_X,train_y,splitter=splitter,groups=groups) model_savgol,result_savgol = opt.optimise(save_file=&#39;models/4_savgot&#39;) . Best model had an MSE of 0.8713 Setting parameters as: {&#39;scaler__window_length&#39;: 3, &#39;scaler__deriv&#39;: 0, &#39;model__alpha&#39;: 0.001} Elapsed time: 31.4681 seconds . Preprocessing Search . So far we&#39;ve introduced our preprocessing techniques (SNV, MSC, Sav-Gol) and found they are pretty dismal for a linear regression. However we need to add the caveat that linear regression does fairly well on this dataset, a moderate amount of regularisation gives reuslts similar to PLS; the preprocessing methods may give better results for other learners. . To investigate this we plug in each preprocessing scheme into a PLS-LWR model. We investigate the following schemes. Suffixed numbers indicate the orer of derivative for Savitsky Golay. . SNV | MSC | SavGol-1 | SavGol-2 | SNV - SavGol-1 | SNV - SavGol-2 | . from lwr import LocalWeightedRegression from skopt.space import Categorical pipe = Pipeline([(&#39;preprocess&#39;,StandardScaler()), (&#39;scaler&#39;,PLSRegression()), (&#39;model&#39;,LocalWeightedRegression()) ]) preprocess_options= [SNV(), MSC(), SavGol(window_length=13,polyorder=2,deriv=1), SavGol(window_length=13,polyorder=2,deriv=2), ] space = [Categorical(preprocess_options,name=&#39;preprocess&#39;), Integer(1,ncol,name=&#39;scaler__n_components&#39;), Integer(1,nrow,name=&#39;model__n_neighbours&#39;), Real(1e-3, 1e3, name=&#39;model__alpha&#39;,prior=&#39;log-uniform&#39;) ] opt = Optimiser(space,pipe,train_X,train_y,splitter=splitter,groups=groups) model_1,result_1 = opt.optimise(save_file=&#39;models/4_pp-pls-lwr1&#39;) . Best model had an MSE of 0.7536 Setting parameters as: {&#39;preprocess&#39;: SavGol(deriv=1, window_length=13), &#39;scaler__n_components&#39;: 40, &#39;model__n_neighbours&#39;: 2059, &#39;model__alpha&#39;: 0.001} Elapsed time: 6335.7100 seconds . pipe = Pipeline([(&#39;preprocess&#39;,SavGol(window_length=13,polyorder=2,deriv=1)), (&#39;scaler&#39;,PLSRegression()), (&#39;model&#39;,LocalWeightedRegression()) ]) space = [Integer(3,21,name=&#39;preprocess__window_length&#39;), Integer(0,2,name=&#39;preprocess__deriv&#39;), Integer(1,ncol,name=&#39;scaler__n_components&#39;), Integer(1,nrow,name=&#39;model__n_neighbours&#39;), Real(1e-3, 1e3, name=&#39;model__alpha&#39;,prior=&#39;log-uniform&#39;) ] opt = Optimiser(space,pipe,train_X,train_y,splitter=splitter,groups=groups) model_2,result_2 = opt.optimise(save_file=&#39;models/4_pp-pls-lwr2&#39;) . Best model had an MSE of 0.7223 Setting parameters as: {&#39;preprocess__window_length&#39;: 9, &#39;preprocess__deriv&#39;: 1, &#39;scaler__n_components&#39;: 71, &#39;model__n_neighbours&#39;: 1285, &#39;model__alpha&#39;: 0.4451583920332214} Elapsed time: 5628.9874 seconds . Summary . In this notebook we have investigated various spectral preprocessing schemes (SNV, MSC, and Savitsky Golay derivatives). We started out by showing how transformed spectra and building linear models on transforemd data. These gave dissapointing results on the mangoes dataset. Using these techniques as an additional preprocessing step for our PLS-LWR model gave more promising results. After first searching across all preprocessing types, the first derivative Savitisky-Golay method gave the most promising results. We refocused our search around this method, finding a set of parameters which set a new best cross-validation MSE of 0.7223. We evaluate this model on the test set below, with an MSE of xyz . model_2,mse_test = evaluate(model_2,train_X,train_y,test_X,test_y,plot=True) . Test set MSE: 0.7686 .",
            "url": "https://huonfraser.github.io/Mangoes/mangoes/2022/06/13/Preprocessing.html",
            "relUrl": "/mangoes/2022/06/13/Preprocessing.html",
            "date": " • Jun 13, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Domain Related Regessions",
            "content": "In this previous part, we introduced the Mangoes data set, performed explanatory analysis for the metadata, defined our evaluation approach and built a linear model. In this part we extend the linear model by using Partial Least Squares (PLS) preprocessing and local sample weightings. . import pathlib import pandas as pd import numpy as np from matplotlib import pyplot as plt from tqdm.notebook import tqdm from codetiming import Timer from sklearn.model_selection import GroupKFold from mangoes_blog.scikit_models import * from skopt.space import Real, Integer from lwr import LocalWeightedRegression from sklearn.pipeline import Pipeline import warnings warnings.filterwarnings(&#39;ignore&#39;) . . mangoes=load_mangoes() train_data,test_data = train_test_split(mangoes) train_X, train_y, train_cat = X_y_cat(train_data,min_X=684,max_X=990) test_X, test_y, test_cat = X_y_cat(test_data,min_X=684,max_X=990) nrow,ncol=train_X.shape groups = train_cat[&#39;Pop&#39;] splitter=GroupKFold() . . Partial Least Squares . PLS is like a supervised equivalent to principal components (PCA) and is widely used for IR-spectra analysis. Like PCA, a change of basis is performed with features transformed along orthooganl axes. The difference between the two is that while PCA selects components that explain variance within X, PLS selects components from X that explain variance within y, making features extracted by PLS more suitable for regression. . We make a small change to the sklearn PLS implementation as it doensn&#39;t play nice with the Pipeline class; being a supervised learner, PLS transforms and returns both X and y in a tuple. Future steps in the pipeline expect only the transformed X values. . from sklearn.cross_decomposition import PLSRegression as PLS_ class PLSRegression(PLS_): def transform(self,X,y=None,copy=True): X = super().transform(X,copy=copy) return X . The optimial model takes 30 components, with a moderate amount of $l_2$ regularisation. Search took around 5 minutes (this varies by run), an order of magnitude more than the search run last post. In future approaches we may want to run PLS only once with 30 components so that for PLS we fit and transform the data only once. . from sklearn.linear_model import Ridge pls= Pipeline([(&#39;scaler&#39;, PLSRegression(n_components=10)), (&#39;model&#39;, Ridge()) ]) space = [Integer(2,ncol,name=&#39;scaler__n_components&#39;), Real(1e-3, 1e3, name=&#39;model__alpha&#39;,prior=&#39;log-uniform&#39;) ] opt = Optimiser(space,pls,train_X,train_y,splitter=splitter,groups=groups) model_pls,result_pls = opt.optimise(save_file=&#39;models/3_pls&#39;) . Best model had an MSE of 0.8116 Setting parameters as: {&#39;scaler__n_components&#39;: 35, &#39;model__alpha&#39;: 0.27337815806191307} Elapsed time: 277.2199 seconds . Locally Weighted Regression . Another method to improve a linear regression is to use lazyily calcualted local instance weights. Rather than training a single, global, model, we define a model that takes unique weights for each each prediction instance. This can be done any number of ways. The approach we take is to use Euclidan distance between and assign the k closest training instances a weight of 1 and the rest a weight of 0. An intuitiv way to understand this is as a kNN model with voting done by linear regression. At train time, we store the training set. At test time, we calculate our closest k-neighbours on which a linear regression is trained. . The advantage of this local weighting approach is that we can avoid outliers in the training data, which may skew a linear model. This approach also better fits non-linear data, as we can create a globally non-linear model from locally linear parts. The downsides of this approach is that we increases the cost of making predictions, the model is less interpretable, and we can run into neighbourhood effects when data is less dense. . We run search with the number of neighbours k ranging from 1 to $ frac{4}{5}nrow$, the size of each training set in cross validation. Note that this is incredibly inefficient for large values of k. We find that a LWR is totally useless here. . from sklearn.preprocessing import StandardScaler lwr= Pipeline([(&#39;scaler&#39;,StandardScaler()), (&#39;model&#39;,LocalWeightedRegression()) ]) space = [Integer(0,1,name=&#39;scaler__with_mean&#39;), Integer(0,1,name=&#39;scaler__with_std&#39;), Real(1e-3, 1e3, name=&#39;model__alpha&#39;,prior=&#39;log-uniform&#39;), Integer(1, nrow*4/5, name=&#39;model__n_neighbours&#39;) ] opt = Optimiser(space,lwr,train_X,train_y,splitter=splitter,groups=groups) model_lwr,result_lwr= opt.optimise(save_file=&#39;models/3_lwr&#39;) . Best model had an MSE of 0.7868 Setting parameters as: {&#39;scaler__with_mean&#39;: 0, &#39;scaler__with_std&#39;: 1, &#39;model__alpha&#39;: 0.0024967728668762105, &#39;model__n_neighbours&#39;: 4668} Elapsed time: 12268.5101 seconds . PLS and LWR . We now combine PLS and LWR models which gives an MSE score of 0.7590. Interestingly our model uses more components (71 rather than 31). . plslwr = Pipeline([(&#39;scaler&#39;,PLSRegression()), (&#39;model&#39;,LocalWeightedRegression()) ]) space = [Integer(1,ncol,name=&#39;scaler__n_components&#39;), Integer(1,nrow*4/5,name=&#39;model__n_neighbours&#39;), Real(1e-3, 1e3, name=&#39;model__alpha&#39;,prior=&#39;log-uniform&#39;) ] opt = Optimiser(space,plslwr,train_X,train_y,splitter=splitter,groups=groups) model_plslwr,result_plslwr = opt.optimise(save_file=&#39;models/3_plslwr&#39;) . Best model had an MSE of 0.7521 Setting parameters as: {&#39;scaler__n_components&#39;: 36, &#39;model__n_neighbours&#39;: 1484, &#39;model__alpha&#39;: 0.09840278280801977} Elapsed time: 5939.1907 seconds . Summary . In the previous notebook we fitted a linear regresssion with a level of $l_2$ regularisation determined by search. The result of this was a model that on the test set gave an MSE of 1.1290. This notebook has built upon this model by using PLS feature extraction and lazy sample weights, a type of approach that is popular for NIR-spectroscopy problems. Neither technique gave good performance alone, but combined we set a new benchmark for cross-validation performance. On the test set his approach scored an MSE of 0.8117, an improvement of -0.3173 over the previous notebook. . plslwr, mse_test = evaluate(plslwr,train_X,train_y,test_X,test_y,plot=True) . Test set MSE: 0.8113 . TypeError Traceback (most recent call last) Input In [8], in &lt;cell line: 2&gt;() 1 mse_test = evaluate(plslwr,train_X,train_y,test_X,test_y,plot=True) -&gt; 2 print(f&#34;Test set MSE: {mse_test:.4f}&#34;) TypeError: unsupported format string passed to tuple.__format__ .",
            "url": "https://huonfraser.github.io/Mangoes/mangoes/2022/06/07/PLS.html",
            "relUrl": "/mangoes/2022/06/07/PLS.html",
            "date": " • Jun 7, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Evaluation",
            "content": "In the previous notebook, we introduced the Mangoes data set, performed explanatory analysis for the metadata, and built a linear model. In this notebook we define our evaluation approach for future experiments. We want to avoid contuinously evaluating on test set as then we may be selecting for models that fit this set well due to chance alone. On the other hand, we need to validate the performance of our models at least occasionally. The middleground approach we will take is to select a single model from each notebook to be evaluated on the test set, with this selected by cross-validation on the training data. . The other element of our evaluation approach is search, we need a method to select hyperparameters. As a starting point we use 50 evaluations for each search. As our models get more complex we may increase this budget, or switch to budgetting based on time. . import sys sys.path.append(&#39;/notebooks/mangoes_blog_public/src/&#39;) model_path = &#39;../models/&#39; import pathlib import pandas as pd import numpy as np from matplotlib import pyplot as plt from sklearn.model_selection import GroupKFold from scikit_models import * from skopt.space import Real, Integer from lwr import LocalWeightedRegression from sklearn.pipeline import Pipeline import warnings warnings.filterwarnings(&#39;ignore&#39;) . . mangoes=load_mangoes() train_data,test_data = train_test_split(mangoes) train_X, train_y, train_cat = X_y_cat(train_data,min_X=684,max_X=990) test_X, test_y, test_cat = X_y_cat(test_data,min_X=684,max_X=990) nrow,ncol=train_X.shape . . Cross-Validation . A ommon cross-validation method for IR-spectra analysis is leave-one-out (in this case fruit), with leave-out-one-population used in applications on Mangoes. However there are concerns that this leads to overfitting, as its likely than for any one particular fruit/pop there is another very similar fruit/pop (As an aside we could measure this by using $kNN_{k=1}$ models, then measuring the deviation between leave-one-out cross-validation scores and train-test scores). Rather we go with the standard 5-fold cross-validation which is a good middle gorund of asking if we can repeat a good level of performance 5 times with decent sized test/validate populations. . The sklearn cross-validate functions (cross_validate, cross_val_score), take averages of scores. This is very practial, as it allows them to nest an evaluation function, however returning functions of scores is not ideal. A better method is to combine the test predictions of each fold into a single set and score based of these. For mean squared error (MSE) this doesn&#39;t make a difference, but for an $R^2$ score, performance may be completely different. It also allows us to plot our results. A requirement for this to work is that each instance in a training set is used for testing only once (s.t. there is one unique prediction for each instance). . Below we define our cross-validation function and an accompanying function to evaluate a model on the test data. . from sklearn.metrics import r2_score, mean_squared_error from sklearn.model_selection import GroupKFold, KFold def cross_validate(model,X,y,splitter=GroupKFold(),groups=None,plot=False,save_loc=None): preds = None ys = None for fold, (inds1,inds2) in enumerate(splitter.split(X,y,groups)): model.fit(X.iloc[inds1,:],y.iloc[inds1,:]) pred = model.predict(X.iloc[inds2,:]) if preds is None: preds = pred ys = y.iloc[inds2,:] else: preds = np.concatenate((preds,pred),axis=0) ys = np.concatenate((ys,y.iloc[inds2,:]),axis=0) r2 = r2_score(ys,preds) mse = mean_squared_error(ys,preds) if plot: ys = ys.flatten() preds = preds.flatten() m, b = np.polyfit(ys, preds, 1) fig, ax = plt.subplots() ls = np.linspace(min(ys),max(ys)) ax.plot(ls,ls*m+b,color = &quot;black&quot;, label = r&quot;$ hat{y}$ = &quot;+f&quot;{m:.4f}y + {b:.4f}&quot;) ax.scatter(x=ys,y=preds,label = r&quot;$R^2$&quot; + f&quot;={r2:.4f}&quot;) ax.set_xlabel(&#39;True Values&#39;) ax.set_ylabel(&#39;Predicted Values&#39;) ax.legend(bbox_to_anchor=(0.5,1)) if not save_loc is None: fig.savefig(save_loc) return mse def evaluate(model,train_X,train_y,test_X,test_y,plot=False,save_loc=None,log=True): test_y=test_y.values.flatten() model.fit(train_X,train_y) preds = model.predict(test_X) r2 = r2_score(test_y,preds) mse = mean_squared_error(test_y,preds) if log: print(f&quot;Test set MSE: {mse:.4f}&quot;) if plot: preds=preds.flatten() m, b = np.polyfit(test_y, preds, 1) fig, ax = plt.subplots() ls = np.linspace(min(test_y),max(test_y)) ax.plot(ls,ls*m+b,color = &quot;black&quot;, label = r&quot;$ hat{y}$ = &quot;+f&quot;{m:.4f}y + {b:.4f}&quot;) ax.scatter(x=test_y,y=preds,label = r&quot;$R^2$&quot; + f&quot;={r2:.4f}&quot;) ax.set_xlabel(&#39;True Values&#39;) ax.set_ylabel(&#39;Predicted Values&#39;) ax.legend(bbox_to_anchor=(0.5,1)) if not save_loc is None: fig.savefig(save_loc) return model, mse . Grouping instance . We use grouped KFold, with grouping done by Pop. As an exercise we show the effect of grouping by comparing to groups by FruitID and no grouping. Mixing populations between folds (grouping by fruit or no grouping) significantly improves validation performance. As the test set includes different populations, this won&#39;t effect the results of the final model, however it may effect our decision of model to select. From here onwards we use grouping by Pop. . Our specific problem is to build a model based of the given populations that can extrapolate to unseen populations, so validating wtih groups based of Pop will give a better indication of the true performance than other methods. . from sklearn.linear_model import LinearRegression model = LinearRegression() #group by pop groups = train_data[&#39;Pop&#39;] splitter = GroupKFold(n_splits=5) print(f&#39;Grouped by Pop, MSE = {cross_validate(model,train_X,train_y,splitter=splitter,groups=groups)}&#39;) #group by Fruit_ID groups = train_data[&#39;Fruit_ID&#39;] splitter = GroupKFold(n_splits=5) print(f&#39;Grouped by Fruit, MSE = {cross_validate(model,train_X,train_y,splitter=splitter,groups=groups)}&#39;) #group by splitter= KFold(shuffle=True) print(f&#39;No Grouping, MSE = {cross_validate(model,train_X,train_y,splitter=splitter,groups=groups)}&#39;) . Grouped by Pop, MSE = 0.8236810194689146 Grouped by Fruit, MSE = 0.6800524561711946 No Grouping, MSE = 0.6748251329051893 . Search . Our task for search is to define a search procudure. Since we&#39;ve defined a custom cross-valdiation method, the sklearn grid_search_cv and random_search cv won&#39;t work. Luckily the scikit-optimize (skopt) package provides a more powerful interface for Bayesian optimisation that we can take advantage of. . We create an interface for the the skopt package, by first defining objective function, which takes a search space and returns a score based on cross-validation results. Any extra parameters for cross validation are defined in initialisation. We then define an optimise function, which wraps our search with nice things like tqdm progress bars and optional outputs This function returns a copy of the model with parameters set to found solution and a dictionary of the search results. All the heavy lifting here is done by the gp_minimize function, which runs Bayesian optimsiation based around a Gaussian process regression kernel. In the future we could extend this to use different kernels. . To create our grid-search and random-search methods, we cheat a little and use the initialisation methods from skopt. Random is just that, while grid search will devide the parameter space evenly to match the number of calls. This approach allows us to run Bayesian, random, and grid search using the same definitions of inputs and recieving the same outputs. . from tqdm import tqdm from codetiming import Timer from skopt import gp_minimize,dump from skopt.space import Real, Integer from skopt.plots import plot_convergence from skopt.utils import use_named_args from skopt.callbacks import VerboseCallback class TqdmCallback(tqdm): def __call__(self, res): super().update() def __getstate__(self): return [] def __setstate__(self, state): pass class Optimiser(): def __init__(self,space,model,X,y,splitter=KFold(),groups=None): self.space=space self.model=model self.X=X self.y=y self.splitter=splitter self.groups=groups def objective(self,**params): self.model.set_params(**params) return cross_validate(self.model, self.X, self.y, splitter=self.splitter,groups=self.groups) def bayesian_optimise(self,n_calls=50,random_state=0): obj = use_named_args(self.space)(self.objective) return gp_minimize(obj,self.space,n_calls=n_calls,random_state=random_state,callback=TqdmCallback(total=n_calls)) def random_optimise(self,n_calls=50, random_state=0): obj = use_named_args(self.space)(self.objective) return gp_minimize(obj,self.space,n_calls=n_calls,n_initial_points=n_calls,initial_point_generator=&#39;random&#39;, random_state=random_state,callback=TqdmCallback(total=n_calls)) def grid_optimise(self,n_calls=50, random_state=0): obj = use_named_args(self.space)(self.objective) return gp_minimize(obj,self.space,n_calls=n_calls,n_initial_points=n_calls,initial_point_generator=&#39;grid&#39;, random_state=random_state,callback=TqdmCallback(total=n_calls)) @Timer() def optimise(self,strategy=&quot;bayesian&quot;,n_calls=50, random_state=0,plot=True,save_file=None,log=True): if strategy==&quot;bayesian&quot;: result = self.bayesian_optimise(n_calls=n_calls, random_state=random_state) elif strategy==&quot;grid&quot;: result = self.grid_optimise(n_calls=n_calls, random_state=random_state) elif strategy==&quot;random&quot;: result = self.random_optimise(n_calls=n_calls, random_state=random_state) #set parameters of model params = {dim.name:result[&#39;x&#39;][i] for i,dim in enumerate(self.space)} model = self.model.set_params(**params) #save model and search if not save_file is None: del result.specs[&#39;args&#39;][&#39;func&#39;] #spaghetti code to not throw an error as the objective function is unserialisable dump(result,save_file+&#39;_search.pkl&#39;) dump(model, save_file+&#39;_model.joblib&#39;) #plot if plot: plot_convergence(result) # log/print results and include a regression plot: if log: print(f&#39;Best model had an MSE of {result.fun:.4f}&#39;) print(f&#39;Setting parameters as: {params}&#39;) if save_file is None: cross_validate(model, self.X,self.y,splitter=self.splitter,groups=self.groups,plot=True,save_loc=None) else: cross_validate(model, self.X,self.y,splitter=self.splitter,groups=self.groups,plot=True,save_loc=save_file+&#39;_plot.png&#39;) return model,result . Search for L2 Regularisation . We run each search variant below for the problem of finding an appropriate level of $l_2$ regularisation to add to a linear regression alongside a standardisation scheme. . To run our search we first define a model, which should be an sklearn type regression model. We then define our search space, with each dimension defined using one of skopt&#39;s provided classes (Integer, Float, Categorical). For each dimension we define the the range of values. For categorical variables this is an explicit, list, while for real values (floats and integers) we defined the start and end range. We can also define log$_{10}$ sampling for numerical values. . from sklearn.preprocessing import StandardScaler from sklearn.linear_model import Ridge pipe = Pipeline([(&#39;scaler&#39;,StandardScaler()), (&#39;model&#39;,Ridge()) ]) space = [ Integer(0,1,name=&#39;scaler__with_mean&#39;), Integer(0,1,name=&#39;scaler__with_std&#39;), Real(1e-3, 1e3, name=&#39;model__alpha&#39;,prior=&#39;log-uniform&#39;) ] groups = train_data[&#39;Pop&#39;] splitter=GroupKFold() opt = Optimiser(space,pipe,train_X,train_y,splitter=splitter,groups=groups) . We demonstrate results for each class of search below. After our optimiser class is declared we can run multiple searches with no overhead. If we are unhappy with the results we can increase the number of calls or attempt a different seed. . print(&#39;Running Grid Search&#39;) model_grid,result_grid= opt.optimise(strategy=&quot;grid&quot;,save_file=model_path+&#39;2_linear_grid&#39;) . Running Grid Search . 100%|██████████| 50/50 [00:09&lt;00:00, 2.79it/s] . Best model had an MSE of 0.8157 Setting parameters as: {&#39;scaler__with_mean&#39;: 1, &#39;scaler__with_std&#39;: 1, &#39;model__alpha&#39;: 0.005623413251903491} Elapsed time: 11.0227 seconds . print(&#39;Running Random Search&#39;) model_rand,result_rand = opt.optimise(strategy=&quot;random&quot;,save_file=model_path+&#39;2_linear_random&#39;) . Running Random Search . 0%| | 0/50 [00:00&lt;?, ?it/s] 2%|▏ | 1/50 [00:00&lt;00:08, 5.92it/s] 4%|▍ | 2/50 [00:00&lt;00:09, 5.17it/s] 6%|▌ | 3/50 [00:00&lt;00:07, 6.07it/s] 8%|▊ | 4/50 [00:00&lt;00:07, 5.86it/s] 10%|█ | 5/50 [00:00&lt;00:07, 5.69it/s] 12%|█▏ | 6/50 [00:01&lt;00:07, 6.05it/s] 14%|█▍ | 7/50 [00:01&lt;00:07, 5.73it/s] 16%|█▌ | 8/50 [00:01&lt;00:07, 5.49it/s] 18%|█▊ | 9/50 [00:01&lt;00:07, 5.39it/s] 20%|██ | 10/50 [00:01&lt;00:07, 5.53it/s] 22%|██▏ | 11/50 [00:01&lt;00:07, 5.44it/s] 24%|██▍ | 12/50 [00:02&lt;00:06, 5.85it/s] 26%|██▌ | 13/50 [00:02&lt;00:06, 5.96it/s] 28%|██▊ | 14/50 [00:02&lt;00:06, 5.67it/s] 30%|███ | 15/50 [00:02&lt;00:05, 5.88it/s] 32%|███▏ | 16/50 [00:02&lt;00:05, 5.94it/s] 34%|███▍ | 17/50 [00:02&lt;00:05, 6.18it/s] 36%|███▌ | 18/50 [00:03&lt;00:05, 5.44it/s] 38%|███▊ | 19/50 [00:03&lt;00:05, 5.36it/s] 40%|████ | 20/50 [00:03&lt;00:05, 5.87it/s] 42%|████▏ | 21/50 [00:03&lt;00:05, 5.79it/s] 44%|████▍ | 22/50 [00:03&lt;00:05, 5.55it/s] 46%|████▌ | 23/50 [00:04&lt;00:04, 5.79it/s] 48%|████▊ | 24/50 [00:04&lt;00:04, 5.62it/s] 50%|█████ | 25/50 [00:04&lt;00:04, 5.74it/s] 52%|█████▏ | 26/50 [00:04&lt;00:04, 5.68it/s] 54%|█████▍ | 27/50 [00:04&lt;00:03, 5.90it/s] 56%|█████▌ | 28/50 [00:04&lt;00:03, 5.92it/s] 58%|█████▊ | 29/50 [00:05&lt;00:03, 5.74it/s] 60%|██████ | 30/50 [00:05&lt;00:03, 6.10it/s] 62%|██████▏ | 31/50 [00:05&lt;00:03, 5.86it/s] 64%|██████▍ | 32/50 [00:05&lt;00:03, 5.92it/s] 66%|██████▌ | 33/50 [00:05&lt;00:02, 6.25it/s] 68%|██████▊ | 34/50 [00:05&lt;00:02, 6.00it/s] 70%|███████ | 35/50 [00:06&lt;00:02, 6.45it/s] 72%|███████▏ | 36/50 [00:06&lt;00:02, 6.13it/s] 74%|███████▍ | 37/50 [00:06&lt;00:02, 5.74it/s] 76%|███████▌ | 38/50 [00:06&lt;00:01, 6.13it/s] 78%|███████▊ | 39/50 [00:06&lt;00:01, 5.88it/s] 80%|████████ | 40/50 [00:06&lt;00:01, 5.78it/s] 82%|████████▏ | 41/50 [00:07&lt;00:01, 5.69it/s] 84%|████████▍ | 42/50 [00:07&lt;00:01, 5.57it/s] 86%|████████▌ | 43/50 [00:07&lt;00:01, 5.80it/s] 88%|████████▊ | 44/50 [00:07&lt;00:01, 5.73it/s] 90%|█████████ | 45/50 [00:07&lt;00:00, 5.83it/s] 92%|█████████▏| 46/50 [00:07&lt;00:00, 6.14it/s] 94%|█████████▍| 47/50 [00:08&lt;00:00, 6.18it/s] 96%|█████████▌| 48/50 [00:08&lt;00:00, 6.13it/s] 98%|█████████▊| 49/50 [00:08&lt;00:00, 6.08it/s] 100%|██████████| 50/50 [00:09&lt;00:00, 3.05it/s] . Best model had an MSE of 0.8180 Setting parameters as: {&#39;scaler__with_mean&#39;: 0, &#39;scaler__with_std&#39;: 1, &#39;model__alpha&#39;: 0.019920586766671106} Elapsed time: 10.6795 seconds . print(&#39;Running Bayesian Search&#39;) model_bayes,result_bayes= opt.optimise(strategy=&quot;bayesian&quot;,save_file=model_path+&#39;2_linear_bayes&#39;) . Running Bayesian Search . 0%| | 0/50 [00:00&lt;?, ?it/s] 2%|▏ | 1/50 [00:00&lt;00:08, 6.11it/s] 4%|▍ | 2/50 [00:00&lt;00:08, 5.80it/s] 6%|▌ | 3/50 [00:00&lt;00:07, 6.41it/s] 8%|▊ | 4/50 [00:00&lt;00:07, 5.81it/s] 10%|█ | 5/50 [00:00&lt;00:08, 5.47it/s] 12%|█▏ | 6/50 [00:01&lt;00:08, 5.34it/s] 14%|█▍ | 7/50 [00:01&lt;00:08, 5.29it/s] 16%|█▌ | 8/50 [00:01&lt;00:08, 5.06it/s] 18%|█▊ | 9/50 [00:01&lt;00:07, 5.17it/s] 20%|██ | 10/50 [00:02&lt;00:11, 3.40it/s] 22%|██▏ | 11/50 [00:02&lt;00:13, 2.88it/s] 24%|██▍ | 12/50 [00:03&lt;00:14, 2.56it/s] 26%|██▌ | 13/50 [00:03&lt;00:15, 2.45it/s] 28%|██▊ | 14/50 [00:04&lt;00:15, 2.37it/s] 30%|███ | 15/50 [00:04&lt;00:15, 2.32it/s] 32%|███▏ | 16/50 [00:04&lt;00:15, 2.26it/s] 34%|███▍ | 17/50 [00:05&lt;00:14, 2.21it/s] 36%|███▌ | 18/50 [00:05&lt;00:14, 2.22it/s] 38%|███▊ | 19/50 [00:06&lt;00:15, 2.03it/s] 40%|████ | 20/50 [00:06&lt;00:14, 2.03it/s] 42%|████▏ | 21/50 [00:07&lt;00:15, 1.91it/s] 44%|████▍ | 22/50 [00:08&lt;00:14, 1.96it/s] 46%|████▌ | 23/50 [00:08&lt;00:13, 1.97it/s] 48%|████▊ | 24/50 [00:09&lt;00:13, 1.93it/s] 50%|█████ | 25/50 [00:09&lt;00:13, 1.83it/s] 52%|█████▏ | 26/50 [00:10&lt;00:12, 1.89it/s] 54%|█████▍ | 27/50 [00:10&lt;00:11, 1.93it/s] 56%|█████▌ | 28/50 [00:11&lt;00:11, 1.88it/s] 58%|█████▊ | 29/50 [00:11&lt;00:12, 1.75it/s] 60%|██████ | 30/50 [00:12&lt;00:12, 1.64it/s] 62%|██████▏ | 31/50 [00:13&lt;00:12, 1.56it/s] 64%|██████▍ | 32/50 [00:14&lt;00:11, 1.51it/s] 66%|██████▌ | 33/50 [00:14&lt;00:11, 1.47it/s] 68%|██████▊ | 34/50 [00:15&lt;00:11, 1.40it/s] 70%|███████ | 35/50 [00:16&lt;00:10, 1.44it/s] 72%|███████▏ | 36/50 [00:16&lt;00:09, 1.52it/s] 74%|███████▍ | 37/50 [00:17&lt;00:09, 1.41it/s] 76%|███████▌ | 38/50 [00:18&lt;00:08, 1.48it/s] 78%|███████▊ | 39/50 [00:18&lt;00:07, 1.50it/s] 80%|████████ | 40/50 [00:19&lt;00:07, 1.42it/s] 82%|████████▏ | 41/50 [00:20&lt;00:06, 1.48it/s] 84%|████████▍ | 42/50 [00:20&lt;00:05, 1.49it/s] 86%|████████▌ | 43/50 [00:21&lt;00:04, 1.50it/s] 88%|████████▊ | 44/50 [00:22&lt;00:04, 1.41it/s] 90%|█████████ | 45/50 [00:23&lt;00:03, 1.41it/s] 92%|█████████▏| 46/50 [00:23&lt;00:02, 1.41it/s] 94%|█████████▍| 47/50 [00:24&lt;00:02, 1.37it/s] 96%|█████████▌| 48/50 [00:25&lt;00:01, 1.40it/s] 98%|█████████▊| 49/50 [00:25&lt;00:00, 1.44it/s] 100%|██████████| 50/50 [00:26&lt;00:00, 1.41it/s] . Best model had an MSE of 0.8157 Setting parameters as: {&#39;scaler__with_mean&#39;: 1, &#39;scaler__with_std&#39;: 1, &#39;model__alpha&#39;: 0.006159146375736338} Elapsed time: 28.0659 seconds . Results across search were indifferent to mean centring, but scaled variance and added a small amount of $l_2$ regularisation. Bayesian and random search converge much quicker than grid search, with Bayesian search returning the best result by a small margin. . Summary . This notebook has introduced our cross validation and search schemes. We used these search schemes to find an optimal $l_2$ regularisation and standardisation scheme. The selected model gave a training MSE of 0.8156. On the test set (shown below) this model scores an MSE of 1.091, a slight improvement upon the 1.1290 (-0.0380) scored by the base linear regression from the last notebook. . model_bayes,mse_test = evaluate(model_bayes,train_X,train_y,test_X,test_y,plot=True,log=True) . Test set MSE: 1.1147 .",
            "url": "https://huonfraser.github.io/Mangoes/mangoes/2022/06/03/Evaluation.html",
            "relUrl": "/mangoes/2022/06/03/Evaluation.html",
            "date": " • Jun 3, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Introduction",
            "content": "Deep learning for tabular data is experiencing a slight renaissance. Several methods have emerged that can compete to boosting methods (such as XGBoost) in favourable conditions. NIR-spectrometry is a branch of tabular data that typically has two properties, large dimensionality and multicollinearity, which in theory make it suited to deep networks. Multilayer Perceptron networks have been used for decades, and recently CNN architectures have been proposed, however, linear regression models and domain related preprocessing are still the predominant technique. For a detailed review of the NIR-spectra I defer to Anderson and Walsh. . Applications in Industry use huge amounts of test instances, the horticuture industry for iinstance automated grading in horticure packhouses. These analysis rely on prior lab analysis for labelling data. As this analysis is relatively expensive (especially for large amounts of data), most datasets are proprietry. Enter the Mangoes dataset (Anderson et al.), a large NIR-spectroscopy dataset for estimating the relationship between spectra and the dry matter, a measure of maturity of mango fruit. . This is first part of a series comparing a range of deep networks to classical approaches the Mangoes data set. The advantage of the Mangoes dataset is that models (and performance metrics) are publicly available for both classical (Anderson et al.) and deep network (Mishra and Passos) approaches. We can start by (re-)producing similar work and once results are inline we can veer off. The plan is to start with simple linear models and gradually adding complexity. Likel we will do the same for deep networks. Currently the following parts are sketched in, with the plan for deep networks being less clear: . 1) Introduction - Exploratory data analysis and a first linear model. 2) Evaluation - Setup our cross validation and search methods. 3) Spectral Regressions - Cover regression techniques commonly used for spectral applications. 4) Spectral Preprocessing - Cover preprocessing techniques commonly used for spectral applications. 5) Black Box tabular - Compare results to ensembles like RandomForest and XGGoost. 6) Deep Networks ... . This notebook introduces the Mangoes data set and performs expoloratory data analysis. These are done in a literate programming style with text, code and outputs alongside each other. We use the nbdev library, code blocks marked with #export are reused in future notebooks, allowing us to incrementally build a library of tools. Occasionally code segments are deemed too complex/large for a notebook and these will be available in the github repository. . Setup . We begin with imports and library settings which we will use for all notebooks. . import pathlib import pandas as pd import numpy as np from matplotlib import pyplot as plt from sklearn.pipeline import Pipeline np.random.seed(123) import warnings warnings.filterwarnings(&#39;ignore&#39;) . . Next we define methods to parse the Mangoes data set. We write a method to load the data (load_mangoes). Training and test splits are provided in the data (for reproducability) and we write a method to seperate these (train_test_split). We then write a method to seperate the spectra data, the target, and other variables into seperate dataframes, with the option to limit the spectra to certain wavelengths. More details about these partitions will be provided below. . def load_mangoes(): mangoes = pd.read_csv(&quot;../data/mangoes_raw.csv&quot;) unique_spectra = mangoes[&#39;DM&#39;].unique() fruit_id_dict = {u:i for i,u in enumerate(unique_spectra)} mangoes[&#39;Fruit_ID&#39;] = mangoes[&#39;DM&#39;].apply(lambda x: fruit_id_dict[x]) return mangoes def train_test_split(data): train_inds = np.logical_not(data[&#39;Set&#39;]==&#39;Val Ext&#39;) test_inds = data[&#39;Set&#39;]==&#39;Val Ext&#39; train_data = data[train_inds] test_data = data[test_inds] return train_data, test_data def X_y_cat(data,min_X=285,max_X=1200): cat_vars=[&#39;Set&#39;,&#39;Season&#39;,&#39;Region&#39;,&#39;Date&#39;,&#39;Type&#39;,&#39;Cultivar&#39;,&#39;Pop&#39;,&#39;Temp&#39;,&#39;Fruit_ID&#39;] y_vars = [&#39;DM&#39;] X_vars = [i for i in data.columns if (not i in y_vars) and (not i in cat_vars)] X_vars = [i for i in X_vars if (int(i)&gt;= min_X) and (int(i)&lt;= max_X)] return data[X_vars], data[y_vars], data[cat_vars] . Running these methods we see that the dataset has 11691 instances, 10243 of which are marked for training and 1448 for testing. . mangoes = load_mangoes() train_data,test_data = train_test_split(mangoes) train_X, train_y, train_cat= X_y_cat(train_data) test_X, test_y, test_cat = X_y_cat(test_data) nrow,ncol=train_X.shape print(f&#39;Data shape: {mangoes.shape}&#39;) print(f&#39;Train data shape: {train_data.shape}&#39;) print(f&#39;Test data shape: {test_data.shape}&#39;) . Data shape: (11691, 316) Train data shape: (10243, 316) Test data shape: (1448, 316) . Taking a step back, below we show a sample of the entire dataset. The first 8 columns are metadata and the 9th, DM, is the target variable. The remaining columns are the spectras, correspnding to readings over 3nm intervals from 285-1200nm. Multiple readings are made for each mango fruit. Each spectra is included twice, corresponding to two spectra readings. Several fruit are read more than twice, for example multiple readings are made at different temperatures. Above we assigned a &#39;Fruit_ID&#39; column based on unique values of DM. This isn&#39;t pefect, different fruit may have the same DM at the precision present in the dataset, or fruit may have different DM values but may be a useful proxy. . print(mangoes.head()) . Set Season Region Date Type Cultivar Pop Temp DM 0 Cal 1 NT 2/10/2015 Hard Green Caly 2 Mid 16.792506 1 Cal 1 NT 2/10/2015 Hard Green Caly 2 Mid 16.792506 2 Cal 1 NT 2/10/2015 Hard Green Caly 2 Mid 16.070979 3 Cal 1 NT 2/10/2015 Hard Green Caly 2 Mid 16.070979 4 Cal 1 NT 2/10/2015 Hard Green Caly 2 Mid 16.394013 285 ... 1176 1179 1182 1185 1188 1191 1194 1197 1200 Fruit_ID 0 0 ... 0 0 0 0 0 0 0 0 0 0 1 0 ... 0 0 0 0 0 0 0 0 0 0 2 0 ... 0 0 0 0 0 0 0 0 0 1 3 0 ... 0 0 0 0 0 0 0 0 0 1 4 0 ... 0 0 0 0 0 0 0 0 0 2 [5 rows x 316 columns] . We also show a selection of spectra from the training portion of the dataset. Missing values in the dataset are coded as 0. Values are missing at each end of the spectrum and are coded as 0. Several columns are completely 0, indicated by the flat lines at each of the below figure. Other columns are partially 0, likely due to different instruments being used to measure different groups of samples. The range of data which includes no missing data is 513-1050nm. Indidently, this is the range that cuts of much of the noise seen at each end. . ax=train_X.mean().plot(label=&#39;Mean&#39;,legend=True) for i in range(0,4): train_X.iloc[i,:].plot(label=i,legend=True) . We also show the distributuon of the target (DM) for the non-test partitions of the data. This is fairly symmetrical and no outliers are present. . train_y.plot.hist(bins=50,density=True) . &lt;AxesSubplot:ylabel=&#39;Frequency&#39;&gt; . Anderson et al. used the 684-990nm range for their analysis, so for the sake of comparison we will also use this range of wavelengths. This range gives significantly smoother data. . train_X, train_y, train_cat = X_y_cat(train_data,min_X=684,max_X=990) test_X, test_y, test_cat = X_y_cat(test_data,min_X=684,max_X=990) print(f&#39;Number of selected spectra variables: {train_X.shape[1]}&#39;) ax=train_X.mean().plot(label=&#39;Mean&#39;,legend=True) for i in range(0,4): train_X.iloc[i,:].plot(label=i,legend=True) . Number of selected spectra variables: 103 . Expoloratory Data Analysis . We now dive into all the non-spectra data present in the dataset. We do this for the training data as we want to avoid learning about the test data set for now. We look at distributions of the target and the mean spectra for each categorical variable. . The Set variable deliminates data into training (cal), tuning (tuning) and test (val ext) sets. Previously we used this variable by combining the cal and tuning sets into a combined training data set. Each set has near identical mean spectra; indicating that these splits can be replaced by any random sampling method. DM distributions vary slightly. . train_data[&#39;DM&#39;].hist(by=train_data[&#39;Set&#39;],bins=30,sharey=True,sharex=True,density =True) train_data.groupby(&#39;Set&#39;)[train_X.columns].mean().transpose().plot() train_data[&#39;Set&#39;].value_counts() . Cal 7413 Tuning 2830 Name: Set, dtype: int64 . Data is taken over 4 growing Seasons. The 4th season is used for the test set, with seasons 1-3 grouped and then divided into training and tuning sets. As for set, pectra readings from season are consistent, while DM varies slightly. . train_data[&#39;DM&#39;].hist(by=train_data[&#39;Season&#39;],bins=30,sharey=True,sharex=True,density =True) train_data.groupby(&#39;Season&#39;)[train_X.columns].mean().transpose().plot() train_data[&#39;Season&#39;].value_counts() . 3 4966 1 3914 2 1363 Name: Season, dtype: int64 . The two growing Regions, NT and QD also display similar spectra readings with slihgtly different DM distributions. . train_data[&#39;DM&#39;].hist(by=train_data[&#39;Region&#39;],bins=30,sharey=True,sharex=True,density =True) train_data.groupby(&#39;Region&#39;)[train_X.columns].mean().transpose().plot() train_data[&#39;Region&#39;].value_counts() . NT 7439 QLD 2804 Name: Region, dtype: int64 . The Physiological stage, or Type, Hard Green or Ripened significantly influences spectra readings and ripened fruit having a much tighter DM distribution. . train_data[&#39;DM&#39;].hist(by=train_data[&#39;Type&#39;],bins=30,sharey=True,sharex=True,density =True) train_data.groupby(&#39;Type&#39;)[train_X.columns].mean().transpose().plot() train_data[&#39;Type&#39;].value_counts() . Hard Green 9151 Ripen 1092 Name: Type, dtype: int64 . Each Cultivar displays a slightly different mean spectrs; cultivars display more variance at the low end and converge towards the high end. DM distributins vary highly. . train_data[&#39;DM&#39;].hist(by=train_data[&#39;Cultivar&#39;],bins=30,sharey=True,sharex=True,density =True) train_data.groupby(&#39;Cultivar&#39;)[train_X.columns].mean().transpose().plot() train_data[&#39;Cultivar&#39;].value_counts() . Caly 2997 KP 2285 HG 1479 R2E2 1207 1243 400 4069 400 1201 398 LadyJ 398 LadyG 348 Keitt 331 Name: Cultivar, dtype: int64 . Mangoes are taken from 94 different Populations (orchards). This variable imacts spectra readings to a greater degree than cultivar. Its likely that there is significant overlap between the cultivar and population variables. . train_data.groupby(&#39;Pop&#39;)[train_X.columns].mean().transpose().plot(legend=False) train_data[&#39;Pop&#39;].value_counts() . 6 308 5 298 10 293 11 276 13 240 ... 43 40 42 39 44 38 40 20 27 10 Name: Pop, Length: 94, dtype: int64 . Readings at different Temperature show the same spectra reading. DM results are identical for each temeperature, which is expected as they share the same lab based label. . train_data[&#39;DM&#39;].hist(by=train_data[&#39;Temp&#39;],bins=30,sharey=True,sharex=True,density =True) train_data.groupby(&#39;Temp&#39;)[train_X.columns].mean().transpose().plot() train_data[&#39;Temp&#39;].value_counts() . No 5870 Low 1480 Mid 1476 High 1417 Name: Temp, dtype: int64 . Explaining Variance . Having examined each categorical variable in turn we now perform a simple variance analysis. We build a linear model related DM to categorical variables. This is done for each combination of categorical variable (with temperature and date excluded), so that by using $R^2$ scores we can calculate the marginal contribution (to percent of variance explained) by each categorical variable. . Iterating through each combination of variables, we see that Pop explains 0.5474% of the variance. Including Cultivar adds an extra 0.002% for a toal of 0.5476%. We conclude that the effect of Region, Type, and Season variables are solely due to aggregates of Pop, or conversely, that the Pop variable aborbs all of the information of these variables. . from sklearn.preprocessing import OneHotEncoder from sklearn.linear_model import LinearRegression from itertools import combinations for i in range(1,6): oh_vars=[&#39;Season&#39;,&#39;Region&#39;,&#39;Type&#39;,&#39;Cultivar&#39;,&#39;Pop&#39;] for selected in list(combinations(oh_vars,i)): selected = list(selected) enc = OneHotEncoder() enc.fit(train_data[selected]) oh_vars1 = enc.transform(train_data[selected]) lin = LinearRegression() lin.fit(oh_vars1,train_y) score = lin.score(oh_vars1,train_y) print(f&#39;R2 score is {score:.4f} for {selected}&#39;) . R2 score is 0.0121 for [&#39;Season&#39;] R2 score is 0.0016 for [&#39;Region&#39;] R2 score is 0.0079 for [&#39;Type&#39;] R2 score is 0.1151 for [&#39;Cultivar&#39;] R2 score is 0.5474 for [&#39;Pop&#39;] R2 score is 0.0131 for [&#39;Season&#39;, &#39;Region&#39;] R2 score is 0.0270 for [&#39;Season&#39;, &#39;Type&#39;] R2 score is 0.1299 for [&#39;Season&#39;, &#39;Cultivar&#39;] R2 score is 0.5474 for [&#39;Season&#39;, &#39;Pop&#39;] R2 score is 0.0175 for [&#39;Region&#39;, &#39;Type&#39;] R2 score is 0.1151 for [&#39;Region&#39;, &#39;Cultivar&#39;] R2 score is 0.5474 for [&#39;Region&#39;, &#39;Pop&#39;] R2 score is 0.1232 for [&#39;Type&#39;, &#39;Cultivar&#39;] R2 score is 0.5474 for [&#39;Type&#39;, &#39;Pop&#39;] R2 score is 0.5476 for [&#39;Cultivar&#39;, &#39;Pop&#39;] R2 score is 0.0378 for [&#39;Season&#39;, &#39;Region&#39;, &#39;Type&#39;] R2 score is 0.1323 for [&#39;Season&#39;, &#39;Region&#39;, &#39;Cultivar&#39;] R2 score is 0.5474 for [&#39;Season&#39;, &#39;Region&#39;, &#39;Pop&#39;] R2 score is 0.1552 for [&#39;Season&#39;, &#39;Type&#39;, &#39;Cultivar&#39;] R2 score is 0.5474 for [&#39;Season&#39;, &#39;Type&#39;, &#39;Pop&#39;] R2 score is 0.5476 for [&#39;Season&#39;, &#39;Cultivar&#39;, &#39;Pop&#39;] R2 score is 0.1260 for [&#39;Region&#39;, &#39;Type&#39;, &#39;Cultivar&#39;] R2 score is 0.5474 for [&#39;Region&#39;, &#39;Type&#39;, &#39;Pop&#39;] R2 score is 0.5476 for [&#39;Region&#39;, &#39;Cultivar&#39;, &#39;Pop&#39;] R2 score is 0.5476 for [&#39;Type&#39;, &#39;Cultivar&#39;, &#39;Pop&#39;] R2 score is 0.1553 for [&#39;Season&#39;, &#39;Region&#39;, &#39;Type&#39;, &#39;Cultivar&#39;] R2 score is 0.5474 for [&#39;Season&#39;, &#39;Region&#39;, &#39;Type&#39;, &#39;Pop&#39;] R2 score is 0.5476 for [&#39;Season&#39;, &#39;Region&#39;, &#39;Cultivar&#39;, &#39;Pop&#39;] R2 score is 0.5476 for [&#39;Season&#39;, &#39;Type&#39;, &#39;Cultivar&#39;, &#39;Pop&#39;] R2 score is 0.5476 for [&#39;Region&#39;, &#39;Type&#39;, &#39;Cultivar&#39;, &#39;Pop&#39;] R2 score is 0.5476 for [&#39;Season&#39;, &#39;Region&#39;, &#39;Type&#39;, &#39;Cultivar&#39;, &#39;Pop&#39;] . To further analyse this we group the data by pop. With the exception of Pop 52, which contains both KP, and LadyG cultivar, each population includes just a single unique value for each categorical value. Variables like Region or Cultivar which may well effect dry matter, we just cannot measure this as their variance is absorbed by Pop. If we wanted to compare say Cultivar and Pop we would need a dataset containing multiple cultivars for each population. . mangoes.groupby(&#39;Pop&#39;).agg({cat_var : &#39;unique&#39; for cat_var in train_cat}) . Set Season Region Date Type Cultivar Pop Temp Fruit_ID . Pop . 1 [Tuning] | [1] | [NT] | [27/09/2015] | [Hard Green] | [Caly] | [1] | [High, Low] | [2863, 2864, 2865, 2866, 2867, 2868, 2869, 287... | . 2 [Cal] | [1] | [NT] | [2/10/2015] | [Hard Green] | [Caly] | [2] | [Mid] | [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,... | . 3 [Cal] | [1] | [NT] | [9/10/2015] | [Hard Green] | [Caly] | [3] | [High, Low, Mid] | [40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 5... | . 4 [Cal] | [1] | [NT] | [22/10/2015] | [Hard Green] | [Caly] | [4] | [High, Low, Mid] | [80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 9... | . 5 [Cal] | [1] | [NT] | [23/10/2015] | [Hard Green] | [KP] | [5] | [High, Low, Mid] | [120, 121, 122, 123, 124, 125, 126, 127, 128, ... | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | . 108 [Val Ext] | [4] | [QLD] | [13/12/2018] | [Hard Green] | [Caly] | [108] | [No] | [4466, 4467, 4468, 4469, 4470, 4471, 4472, 447... | . 109 [Val Ext] | [4] | [QLD] | [19/01/2019] | [Ripen] | [HG] | [109] | [No] | [4506, 4507, 4508, 4509, 4510, 4511, 4512, 451... | . 110 [Val Ext] | [4] | [QLD] | [22/01/2019] | [Ripen] | [Caly] | [110] | [No] | [4546, 4547, 4548, 4549, 4550, 4551, 4552, 455... | . 111 [Val Ext] | [4] | [QLD] | [25/01/2019] | [Ripen] | [Caly] | [111] | [No] | [4586, 4587, 4588, 4589, 4590, 4591, 4592, 459... | . 112 [Val Ext] | [4] | [QLD] | [25/01/2019] | [Ripen] | [HG] | [112] | [No] | [4626, 4627, 4628, 4629, 4630, 4631, 4632, 463... | . 112 rows × 9 columns . A Linear Model and Summary . This notebook has introduced the mangoes dataset and performed simple expoloratory analysis, finding that the Pop variable explains over half the variance in the dataset. This has implications for test set performance, with the training and test sets consisting of distinct sets of populations. . In the next few notebooks we will get into the meat and potatoes of building regression models. To finish this notebook off we train a linear model on th training portion of the data, which gives an MSE score of 1.1290 on the test set. . from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error model = LinearRegression() model.fit(train_X,train_y) preds = model.predict(test_X) mse = mean_squared_error(test_y,preds) print(f&quot;Test set MSE is: {mse:.4f}&quot;) . Test set MSE is: 1.1290 .",
            "url": "https://huonfraser.github.io/Mangoes/mangoes/2022/06/01/Introduction.html",
            "relUrl": "/mangoes/2022/06/01/Introduction.html",
            "date": " • Jun 1, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://huonfraser.github.io/Mangoes/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://huonfraser.github.io/Mangoes/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}