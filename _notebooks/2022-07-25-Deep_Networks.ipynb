{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d6494d1-633a-4013-bae6-c1231faeef65",
   "metadata": {},
   "source": [
    "# Deep Networks (Draft)\n",
    "> Part 5 of the mangoes_blog project\n",
    "\n",
    "- branch: master\n",
    "- toc: true \n",
    "- badges: false\n",
    "- comments: false\n",
    "- sticky_rank: 5\n",
    "- author: Huon Fraser\n",
    "- categories: [mangoes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "195801bb-bc64-4899-9a22-f84bcd056b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "import sys\n",
    "sys.path.append('/notebooks/Mangoes/src/')\n",
    "model_path  = '../models/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90550f5-b2c3-447a-a6d6-1458fc6fa3f9",
   "metadata": {},
   "source": [
    "## Cross-Validation\n",
    "\n",
    "So far we have implemented a minimum-viable model and evaluation. We now wrap this into a cross-validation framework.\n",
    "\n",
    "First we need to consider a key design choice. For our sklearn models, cross-validation led to building multiple versions of a model with slightly different parameters. For neural networks, with optimising being a highly stochastic gradient descent path, there is no gurantee that cross-validation folds are similar, or that folds resemble the final model trained on all the data. After running cross-validation, we let the user define how to build the final model; None, for building no final model to save time, \"All\", to train a model on the whole training set, or \"Ensemble\", to build an ensemble on the cross-validation folds. \n",
    "\n",
    "We define our ensemble implementation below. At the moment we just pass in each model. IN the future we may instead pass in a location of a savefile for each model, or a single model and a list of locations to get weights from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fd700f-6b0b-45e1-afed-02570177d10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EnsembleConfig(ModelConfig):\n",
    "    models: list  = []\n",
    "    voting : str = \"Mean\"\n",
    "\n",
    "class Ensemble(EnsembleModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: DictConfig,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(config, **kwargs)\n",
    "\n",
    "    def _build_network(self):\n",
    "        self.models = self.hparams[\"continuous_dim\"]\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = x[\"continuous\"]\n",
    "        y_hats=torch.zeros(x.shape[1],len(self.models))\n",
    "        for i,model in enumerate(models):\n",
    "            y_hats[:,i]= self.model.forward(x)['logits']\n",
    "        if voting == \"Mean\":\n",
    "            y_hat = torch.mean(y_hats)\n",
    "        return  {'logits':y_hat}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a14f8d-f419-499e-a61a-24cbcff30128",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold, KFold\n",
    "\n",
    "def cross_validate(model,X,y,splitter=GroupKFold(),groups=None,plot=False,save_loc=None,final_model=\"Ensemble\"): #Ensemble, \"All\", None\n",
    "    #combine X and y\n",
    "    Xy = deepcopy(X)\n",
    "    Xy[\"target\"]=y\n",
    "    \n",
    "    preds = None\n",
    "    ys = None\n",
    "    models = []\n",
    "    for fold, (inds1,inds2) in enumerate(splitter.split(X,y,groups)):\n",
    "        \n",
    "        model.fit(X.iloc[inds1,:],y.iloc[inds1,:])\n",
    "        pred = model.predict(X.iloc[inds2,:])\n",
    "\n",
    "        if preds is None:\n",
    "            preds = pred\n",
    "            ys = y.iloc[inds2,:]\n",
    "        else:\n",
    "            preds = np.concatenate((preds,pred),axis=0)\n",
    "            ys = np.concatenate((ys,y.iloc[inds2,:]),axis=0)\n",
    "            \n",
    "        if final_model == \"Ensemble\":\n",
    "            models.append(deepcopy(model))\n",
    "            \n",
    "\n",
    "    r2 = r2_score(ys,preds)\n",
    "    mse = mean_squared_error(ys,preds)\n",
    "\n",
    "    if plot:\n",
    "        ys = ys.flatten()\n",
    "        preds = preds.flatten()\n",
    "\n",
    "        m, b = np.polyfit(ys, preds, 1)\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "        ls = np.linspace(min(ys),max(ys))\n",
    "        ax.plot(ls,ls*m+b,color = \"black\", label = r\"$\\hat{y}$ = \"+f\"{m:.4f}y + {b:.4f}\")\n",
    "        ax.scatter(x=ys,y=preds,label = r\"$R^2$\" + f\"={r2:.4f}\")\n",
    "\n",
    "        ax.set_xlabel('True Values')\n",
    "        ax.set_ylabel('Predicted Values')\n",
    "        ax.legend(bbox_to_anchor=(0.5,1))\n",
    "        if not save_loc is None:\n",
    "            fig.savefig(save_loc)\n",
    "            \n",
    "    if final_model == \"Ensemble\":\n",
    "         #create new tabular model, passing in same configs but with an ensemble\n",
    "            \n",
    "        ens_config = EnsembleConfig(models=models)\n",
    "            \n",
    "        ensemble = TabularModel(\n",
    "            data_config=model.data_config,\n",
    "            optimizer_config=model.optimizer_config,\n",
    "            trainer_config=model.trainer_config,\n",
    "            model_config=ens_config,\n",
    "            model_callable = Ensemble\n",
    ")\n",
    "        model = ensemble\n",
    "    elif final_model == \"All\": #train final model on all data\n",
    "        model = model.fit(train=Xy)\n",
    "    else: #ignore training the final model, for computation saving purposes \n",
    "        model = None\n",
    "    \n",
    "    return model, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a8e6f2-fef4-43d3-a049-3ed9729d0bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model,train_X,train_y,test_X,test_y,plot=False,save_loc=None,log=True):\n",
    "    test_y=test_y.values.flatten()\n",
    "    model.fit(train_X,train_y)\n",
    "    preds = model.predict(test_X)\n",
    "\n",
    "    r2 = r2_score(test_y,preds)\n",
    "    mse = mean_squared_error(test_y,preds)\n",
    "\n",
    "    if log:\n",
    "        print(f\"Test set MSE: {mse:.4f}\")\n",
    "\n",
    "    if plot:\n",
    "        preds=preds.flatten()\n",
    "\n",
    "        m, b = np.polyfit(test_y, preds, 1)\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "        ls = np.linspace(min(test_y),max(test_y))\n",
    "        ax.plot(ls,ls*m+b,color = \"black\", label = r\"$\\hat{y}$ = \"+f\"{m:.4f}y + {b:.4f}\")\n",
    "        ax.scatter(x=test_y,y=preds,label = r\"$R^2$\" + f\"={r2:.4f}\")\n",
    "\n",
    "        ax.set_xlabel('True Values')\n",
    "        ax.set_ylabel('Predicted Values')\n",
    "        ax.legend(bbox_to_anchor=(0.5,1))\n",
    "        if not save_loc is None:\n",
    "            fig.savefig(save_loc)\n",
    "    return model, mse "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
