{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c153ea0-7d71-4dfd-be48-f5fc1b71370d",
   "metadata": {},
   "source": [
    "# Wrapping up Linear Models\n",
    "> Part 5 of the mangoes_blog project\n",
    "\n",
    "- branch: master\n",
    "- toc: true \n",
    "- badges: false\n",
    "- comments: false\n",
    "- sticky_rank: 5\n",
    "- author: Huon Fraser\n",
    "- categories: [mangoes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3e9b7ee-07d0-46c0-b0a4-bb42c77d81b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "C extension: numpy.core.multiarray failed to import not built. If you want to import pandas from the source directory, you may need to run 'python setup.py build_ext --inplace --force' to build the C extensions first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m__init__.pxd:942\u001b[0m, in \u001b[0;36mnumpy.import_array\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0x10 but this version of numpy is 0xf",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/__init__.py:30\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hashtable \u001b[38;5;28;01mas\u001b[39;00m _hashtable, lib \u001b[38;5;28;01mas\u001b[39;00m _lib, tslib \u001b[38;5;28;01mas\u001b[39;00m _tslib\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# hack but overkill to use re\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/_libs/__init__.py:13\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaT\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaTType\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterval\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m ]\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterval\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interval\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtslibs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     15\u001b[0m     NaT,\n\u001b[1;32m     16\u001b[0m     NaTType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     iNaT,\n\u001b[1;32m     22\u001b[0m )\n",
      "File \u001b[0;32mpandas/_libs/interval.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.interval\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.hashtable\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/missing.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.missing\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/_libs/tslibs/__init__.py:30\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dtypes\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OutOfBoundsTimedelta, localize_pydatetime\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Resolution\n",
      "File \u001b[0;32mpandas/_libs/tslibs/conversion.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.tslibs.conversion\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/tslibs/nattype.pyx:27\u001b[0m, in \u001b[0;36minit pandas._libs.tslibs.nattype\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m__init__.pxd:944\u001b[0m, in \u001b[0;36mnumpy.import_array\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: numpy.core.multiarray failed to import",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#collapse-hide\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/__init__.py:34\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# hack but overkill to use re\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot import name \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC extension: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not built. If you want to import \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpandas from the source directory, you may need to run \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpython setup.py build_ext --inplace --force\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to build the C extensions first.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     38\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     41\u001b[0m     get_option,\n\u001b[1;32m     42\u001b[0m     set_option,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     46\u001b[0m     options,\n\u001b[1;32m     47\u001b[0m )\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# let init-time option registration happen\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: C extension: numpy.core.multiarray failed to import not built. If you want to import pandas from the source directory, you may need to run 'python setup.py build_ext --inplace --force' to build the C extensions first."
     ]
    }
   ],
   "source": [
    "#collapse-hide\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append('/notebooks/Mangoes/src/')\n",
    "model_path  = '../models/'\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from codetiming import Timer\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from scikit_models import *\n",
    "from skopt.space import Real, Integer\n",
    "from lwr import LocalWeightedRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c346273-0635-441e-b1af-6f8b1ede1607",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse-hide\n",
    "mangoes=load_mangoes()\n",
    "\n",
    "train_data,test_data = train_test_split(mangoes)\n",
    "train_X, train_y, train_cat = X_y_cat(train_data,min_X=684,max_X=990)\n",
    "test_X, test_y, test_cat = X_y_cat(test_data,min_X=684,max_X=990)\n",
    "nrow,ncol=train_X.shape\n",
    "groups = train_cat['Pop']\n",
    "splitter=GroupKFold()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beef5db8-d73c-453b-b92c-a7dda3583dca",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Ensemble methods\n",
    "\n",
    "We now compare our approach to off the shelf ensemble models that are typically the state of the art for tabular problems. We train off-the-shelf variants of [Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html), the default sklearn [Gradient Boosting Regression](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor) and [XGBoost](https://xgboost.readthedocs.io/en/stable/). A caveat here is that each of these models could probably be optimised further. Intitial performance with standaridisation preprocessing was dissapointing so we used PLS preprocessing for these experiemnts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53077d3c-1d49-4bef-86e0-2b589b455e1c",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c72892-54ee-462c-92c2-90ac274d7dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = Pipeline([\n",
    "    ('scaler', PLSRegression()),\n",
    "    ('model',RandomForestRegressor())\n",
    "    ])\n",
    "\n",
    "space  = [Integer(2,ncol,name='scaler__n_components'),\n",
    "         ]\n",
    "\n",
    "opt = Optimiser(space,model,train_X,train_y,splitter=splitter,groups=groups)\n",
    "model_forest,result_forest = opt.optimise(save_file=model_path+'5_random_forest')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3109b3d-566d-4f51-a782-fa2886fdf58f",
   "metadata": {},
   "source": [
    "### Scikit-learn Gradient Boosting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623a8a3c-7fa5-4068-8eda-b0e3d363a9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "model = Pipeline([\n",
    "    ('scaler', PLSRegression()),\n",
    "    ('model',GradientBoostingRegressor(random_state=0))\n",
    "    ])\n",
    "\n",
    "\n",
    "space  = [Integer(2,ncol,name='scaler__n_components'),\n",
    "         ]\n",
    "\n",
    "opt = Optimiser(space,model,train_X,train_y,splitter=splitter,groups=groups)\n",
    "model_boost,result_boost = opt.optimise(save_file=model_path+'5_gradboost')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f7fd6f-510a-483b-80a4-d4ddeadf3396",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b44a255-ff27-42f1-b297-24eeb31f5add",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "model = Pipeline([\n",
    "    ('scaler', PLSRegression()),\n",
    "    ('model',xgb.XGBRegressor(tree_method=\"gpu_hist\"))\n",
    "    ])\n",
    "space = [\n",
    "        Integer(2,ncol,name='scaler__n_components')\n",
    "        ]\n",
    "\n",
    "opt = Optimiser(space,model,train_X,train_y,splitter=splitter,groups=groups)\n",
    "model_xg, result_xg = opt.optimise(save_file=model_path+'5_xgboost')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57313902-7a8c-42bd-89ec-3419905e7e29",
   "metadata": {},
   "source": [
    "### Ensembles of  PLS-LWR\n",
    "\n",
    "We've left them until last because typically emsembles will always give better performance; any of the models looked at in the previous 3 parts could be ensmbled. We take our previous best model (SG-PLS-LWR) and build a [bagging regressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html) ensemble. This builds 10 copys of a model, with each trained on a sample (with replacement) of the dataset. Predictions are then made by taking the mean of the ensemble.\n",
    "\n",
    "Results are no better than for the non-ensembled version. A possible explanation is that bagging reduces the density of the feature space, interfering with the locally weighted regressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841ce046-7250-4cc0-9742-f73d5cef3901",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "from joblib import dump, load\n",
    "\n",
    "pipe =  load(model_path+'4_pp-pls-lwr_model2.joblib') \n",
    "model = BaggingRegressor(pipe)\n",
    "\n",
    "mse = cross_validate(model,train_X,train_y,splitter=splitter,groups=groups,plot=True)\n",
    "print(f'Train set MSE: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6ff794-7511-4723-beb7-a6323fe855c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, mse_test = evaluate(model,train_X,train_y,test_X,test_y,plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3f5eac-4d5e-4a51-ba74-121e61549213",
   "metadata": {},
   "source": [
    "## Comparing Techniques\n",
    "\n",
    "\n",
    "So far in this series, we started with a multiple linear regression (LR) and dded complexity; feature extraction with partial least squares (PLS), lazy instance weights with locally weighted regressions (LWR) and preprocessing with Savitsky Golay (SG). Adding ach of these components incrementally improved performance during cross-validation, although the hyperparameter settings were not always consistent. The final model in this series (SG-PLS-LWR) gave a cross-validation MSE of 0.7223 and a test MSE of 0.7686.\n",
    "\n",
    "When we compared this model to off-the-shelf ensembles (including XGBoost) and a bagging-ensemble extension, we found that these underperformed our model. To round out this part of the series we compare our results to the original Mangoes results achieved by Anderson et al. \n",
    "\n",
    "In the table we below we compare two models by Anderson et al, LPLS, their best performing locally weighted PLS model, and Ensemble, their best ensemble based model to the models we have built in this series. Our approach gave substantially better results that both models.  Without going into too much detail, this is likely due to the Anderson et al. models fixing the number of components for PLS to a relatively low number, whereas we kept this hyperparmater flexible.\n",
    "\n",
    "| Model         | CV Score        | Test Score  |\n",
    "| ------------  | :-------------: | -----:      |\n",
    "| LR            | 0.8157          | 1.1147 |\n",
    "| PLS-LR        | 0.8116          | -------  |\n",
    "| LWR           | 0.7868          | -------|\n",
    "| PLS-LWR       | 0.7520          | 0.8113  |\n",
    "| SG-PLS-LWR    | 0.7223          | 0.7686 |\n",
    "| E(SG-PLS-LWR) | 0.7236          | 0.7801   |\n",
    "| Anderson et al. LPLS   | 0.66   | 0.887   |\n",
    "| Anderson et al. Ensemble | 0.56 | 0.850  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beb99ed-9744-4187-80fa-f3c8490f4492",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
